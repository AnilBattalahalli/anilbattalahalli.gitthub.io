[{"content":" $$ \\gdef{\\boldphi}{\\boldsymbol{\\phi}} \\gdef{\\xmin} {x_{\\text{min}}} \\gdef{\\gammaone} {\\Gamma_{\\alpha}^{\\prime} (1-\\alpha, \\lambda \\xmin)} \\gdef{\\gammatwo}{\\Gamma_{\\alpha}^{\\prime \\prime} (1-\\alpha, \\lambda \\xmin)} \\gdef{\\gammazero}{\\Gamma (1-\\alpha, \\lambda \\xmin)} $$ Introduction The power-law distribution is of the form \\( f(x) \\propto x^{-\\alpha} \\), where \\( \\alpha \\) is called the scaling parameter. It models many natural phenomena like acoustic attenuation, Curie–Von Schweidler law, neuronal avalanches, and others. As \\( x \\to 0 \\), \\( f(x) \\) diverges out of bounds. Hence, we define \\( \\xmin \\) as a lower bound for the support of the distribution function \\( f(x) \\). Using this, the power-law density function can be written as\n$$ f(x) = \\frac{\\alpha-1}{\\xmin}\\left( \\frac{x}{\\xmin}\\right)^{-\\alpha} \\ \\ \\ \\ , \\ x \u003e \\xmin $$ More generally, the power-law distribution function can be written with an exponential cutoff which is of the form \\( f(x) \\propto x^{-\\alpha}e^{-\\lambda x} \\) , which is again defined for \\( x \u0026gt; \\xmin \\). While the estimation of the scaling parameter is quite straightforward with maximum likelihood estimation, or by using a variant of MLE, the estimation of the lower-bound parameter \\( \\xmin \\) is quite challenging as it can be overestimated in favor of a high likelihood value. One way to combat this problem is by penalizing the likelihood function for higher \\( \\xmin \\), as discussed by Olmez, et.al. Another way to solve this is by using the Bayesian approach, which allows us to impose an appropriate prior on \\( \\xmin \\). In this research, I discuss the estimation of the parameters of a power-law distribution with an exponential cutoff in three cases -\nUnknown \\( \\alpha \\), and known \\( \\xmin \\), \\( \\lambda \\) Unknown \\( \\xmin \\) and \\( \\alpha \\), and known \\( \\lambda \\) Unknown \\( \\alpha \\), \\( \\lambda \\), and \\( \\xmin \\) Methods This section discusses the establishment of the Bayesian framework for the distribution, for the three cases involving different priors.\nProbability Density Function $$ f(x) = C x^{-\\alpha}e^{-\\lambda x} 1(x \u003e \\xmin) $$ It can be easily verified using simple integration: \\( C = \\lambda^{1-\\alpha} \\ / \\ \\Gamma(1-\\alpha, \\lambda \\xmin) \\ \\ \\) where, \\( \\Gamma(1-\\alpha, \\lambda \\xmin) \\) is the upper incomplete gamma function defined as,\n$$ \\Gamma(s,x) = \\int_{x}^{\\infty} t^{s-1} e^{-t} dt \\ \\ \\ \\ , \\ x \u003e 0 $$ Bayesian Framework A general form of the posterior distribution of the parameter space \\( \\boldphi \\) conditioned on the given data \\( \\mathcal{D} \\) is given by\n$$ p(\\boldphi \\ | \\ \\mathcal{D}) \\propto_{\\boldphi} \\mathcal{L}(\\mathcal{D}, \\boldphi) \\ p(\\boldphi) $$ where \\( \\mathcal{D} = { x_1, x_2, \\dots, x_n } \\)\n\\( \\mathcal{L}(\\mathcal{D}, \\boldphi) \\) is the likelihood function for the samples given the parameters in the parameter space. In the case of this distribution, the existence of \\( \\xmin \\) and the same being a parameter to be estimated (random) adds to the complexity of defining the likelihood function of the data given the parameters \\( \\boldphi = (\\alpha, \\lambda, \\xmin) \\).\n$$ \\begin{aligned} \\mathcal{L}(\\mathcal{D}, \\boldphi) \u0026= \\prod_{x_i \u003e \\xmin} f(x_i|\\boldphi) \\end{aligned} $$ We know that \\( 0 \u0026lt; f(x_i \\ | \\alpha, \\lambda, \\xmin) \u0026lt; 1, \\ \\forall x_i \u0026gt; \\xmin \\), and hence, for a given \\( \\mathcal{D} = {x_1, x_2, \\dots, x_n} \\), \\( \\prod_{x_i \u0026gt; \\xmin} f(x_i \\ | \\alpha, \\lambda, \\xmin) \\) decreases monotonically with \\( \\xmin \\). To counter this problem, an appropriate prior must be specified for \\( \\xmin \\).\n$$ \\begin{aligned} \\mathcal{L}(\\mathcal{D}, \\boldphi) \u0026= \\prod_{x_i \u003e \\xmin} \\frac{\\lambda^{1-\\alpha}} {\\Gamma(1-\\alpha, \\lambda \\xmin)} \\ x_i^{-\\alpha} e^{-\\lambda x_i} \\ 1(x_i \u003e \\xmin) \\\\ \u0026= \\prod_{x_i \u003e \\xmin} \\frac{\\lambda^{1-\\alpha}} {\\Gamma(1-\\alpha, \\lambda \\xmin)} \\ x_i^{-\\alpha} e^{-\\lambda x_i} \\\\ \u0026\\equiv \\prod_{i = 1}^n \\left[ \\frac{\\lambda^{1-\\alpha}} {\\Gamma(1-\\alpha, \\lambda \\xmin)} \\ x_i^{-\\alpha} e^{-\\lambda x_i} 1(x_i \u003e \\xmin) + 1(x_i \\le \\xmin) \\right] \\\\ \\mathcal{L}(\\mathcal{D}, \\boldphi) \u0026= \\prod_{i = 1}^n \\left[ \\frac{\\lambda^{1-\\alpha}} {\\Gamma(1-\\alpha, \\lambda \\xmin)} \\ x_i^{-\\alpha} e^{-\\lambda x_i} 1(x_i \u003e \\xmin) + 1(x_i \\le \\xmin) \\right] \\end{aligned} $$ The Bayesian framework can now be written as\n$$ p(\\boldphi \\ | \\ \\mathcal{D}) \\propto_{\\boldphi} \\prod_{i = 1}^n \\left[ \\frac{\\lambda^{1-\\alpha}} {\\Gamma(1-\\alpha, \\lambda \\xmin)} \\ x_i^{-\\alpha} e^{-\\lambda x_i} 1(x_i \u003e \\xmin) + 1(x_i \\le \\xmin) \\right]\\ \\ p(\\boldphi) $$ The prior distribution of \\( \\boldphi \\) consists of joint prior of \\( \\alpha \\), \\( \\lambda \\) and \\( \\xmin \\).\nJeffrey\u0026rsquo;s Prior for the Scaling Parameter Jeffrey\u0026rsquo;s prior is a non-informative prior. It does not make modeling assumptions of the parameter, and the posterior function thus obtained represents the parameter given the data as best as possible.\nJeffrey\u0026rsquo;s prior is given by,\n$$ \\xi^{J}(\\phi) \\propto_{\\phi} \\sqrt{I^{F}(\\phi)} $$ Here, $I^{F}(\\phi)$ is the Fischer information given by,\n$$ \\begin{aligned} \u0026I^{F}(\\phi) = E_{[X|\\phi]} \\left\\{ \\frac{\\partial}{\\partial \\phi} \\log f(X|\\phi) \\right\\}^2 \\\\ \u0026\\text{or}, \\\\ \u0026I^{F}(\\phi) = - \\ E_{[X|\\phi]} \\left\\{ \\frac{\\partial^2}{\\partial \\phi^2} \\log f(X|\\phi) \\right\\} \\end{aligned} $$ To obtain the Jeffrey\u0026rsquo;s prior for the scaling parameter, we need\n$$ \\begin{aligned} I^{F}(\\alpha) \u0026amp;= - \\ E_{[X|\\alpha]} \\left\\{ \\frac{\\partial^2}{\\partial \\alpha^2} \\log f(X|\\alpha) \\right\\}\\\\ \u0026amp;= - \\ E_{[X|\\alpha]} \\frac{\\partial^2}{\\partial \\alpha^2} \\log \\left[ \\frac{\\lambda^{1-\\alpha}} {\\Gamma(1-\\alpha, \\lambda \\xmin)} \\ x^{-\\alpha} e^{-\\lambda x} \\ 1(x \u0026gt; \\xmin) \\right] \\\\ \u0026amp;= - \\ E_{[X|\\alpha]} \\frac{\\partial^2}{\\partial \\alpha^2} \\left[ -\\alpha \\log x - \\lambda x +(1-\\alpha) \\log \\lambda + \\log1(x\u0026gt;\\xmin)- \\log \\Gamma(1-\\alpha, \\lambda \\xmin) \\right] \\\\ \u0026amp;= - \\ E_{[X|\\alpha]} \\frac{\\partial}{\\partial \\alpha} \\left[ -\\log x - \\log \\lambda - \\frac{\\Gamma_{\\alpha}^{\\prime} (1-\\alpha, \\lambda \\xmin)}{\\Gamma_{\\alpha} (1-\\alpha, \\lambda \\xmin)} \\right]\\\\ \u0026amp;= - \\ E_{[X|\\alpha]} \\left[\\frac {\\gammaone^2 \\ - \\ [\\gammazero \\ \\gammatwo]}{\\gammazero^2} \\right]\\\\ I^{F}(\\alpha) \u0026amp;= \\frac {[\\gammazero \\ \\gammatwo] \\ - \\ \\gammaone^2 }{\\gammazero^2} \\end{aligned} $$ Here, \\( \\gammaone \\) is the first partial derivative of \\( \\gammazero \\) with respect to \\( \\alpha \\), \\( \\frac{\\partial}{\\partial \\alpha} \\gammazero \\), and \\( \\gammatwo \\) is the second partial derivative of \\( \\gammazero \\) with respect to \\( \\alpha \\), \\( \\frac{\\partial^2}{\\partial \\alpha^2} \\gammazero \\).\n$$ \\begin{aligned} \\gammaone \u0026amp;= \\frac{\\partial}{\\partial \\alpha} \\gammazero \\\\ \u0026amp;= \\frac{\\partial}{\\partial \\alpha} \\left[ \\int_{\\lambda \\xmin}^{\\infty} t^{-\\alpha} e^{-t} dt \\right]\\\\ \u0026amp;= \\int_{\\lambda \\xmin}^{\\infty} \\frac{\\partial}{\\partial \\alpha} (t^{-\\alpha}) e^{-t} dt \\\\ \u0026amp;= \\int_{\\lambda \\xmin}^{\\infty} -\\alpha t^{-\\alpha-1} e^{-t} dt \\\\ \\gammaone \u0026amp;= -\\alpha \\Gamma(-\\alpha, \\lambda \\xmin)\\\\ \\\\ \\gammatwo \u0026amp;= \\frac{\\partial}{\\partial \\alpha} [-\\alpha \\Gamma(-\\alpha, \\lambda \\xmin)]\\\\ \u0026amp;= -\\Gamma(-\\alpha, \\lambda \\xmin) - \\alpha \\int_{\\lambda \\xmin}^{\\infty} (-\\alpha-1) t^{-\\alpha-2} e^{-t} dt \\\\ \\gammatwo \u0026amp;= \\alpha (\\alpha + 1) \\Gamma(-1-\\alpha, \\lambda \\xmin) -\\Gamma(-\\alpha, \\lambda \\xmin) \\end{aligned} $$ Hence, our Jeffrey\u0026rsquo;s prior can be simplified to,\n$$ \\begin{aligned} \\xi^{J}(\\phi) \u0026amp;\\propto_{\\phi} \\sqrt{I^{F}(\\phi)}\\\\ \u0026amp;\\propto_{\\phi} \\sqrt{\\frac {[\\gammazero \\ \\gammatwo] \\ - \\ \\gammaone^2 }{\\gammazero^2} } \\\\ \\xi^{J}(\\alpha | \\ \\lambda, \\xmin) \u0026amp;\\propto_{\\alpha} \\sqrt{\\frac {[\\gammazero \\ \\{\\alpha (\\alpha + 1) \\Gamma(-1-\\alpha, \\lambda \\xmin) -\\Gamma(-\\alpha, \\lambda \\xmin)\\}] \\ - \\ (\\alpha \\Gamma(-\\alpha, \\lambda \\xmin))^2 }{\\gammazero^2}} \\end{aligned} $$ Priors for the three cases -\nKnown \\( \\xmin \\), \\( \\lambda \\), and unknown \\( \\alpha \\): In this case, we can use Jeffrey\u0026rsquo;s prior on \\( \\alpha \\) as it is the only parameter to be estimated. Our posterior is of the form, $$ p(\\boldphi \\ | \\ \\mathcal{D}) \\propto_{\\boldphi} \\prod_{i = 1}^n \\left[ \\frac{\\lambda^{1-\\alpha}} {\\Gamma(1-\\alpha, \\lambda \\xmin)} \\ x_i^{-\\alpha} e^{-\\lambda x_i} 1(x_i \u003e \\xmin) + 1(x_i \\le \\xmin) \\right] \\xi^{J}(\\alpha) $$ where \\( \\xi^{J}(\\alpha) \\) is the Jeffrey\u0026rsquo;s prior at \\( \\alpha \\)\nKnown \\( \\lambda \\), and unknown \\( \\xmin \\) and \\( \\alpha \\): In this case, we can use a uniform prior on \\( \\alpha \\), and an exponential prior with lower and upper bounds on \\( \\xmin \\) which would discourage higher values of \\( \\xmin \\) on the posterior as discussed before. $$ p(\\boldphi \\ | \\ \\mathcal{D}) \\propto_{\\boldphi} \\prod_{i = 1}^n \\left[ \\frac{\\lambda^{1-\\alpha}} {\\Gamma(1-\\alpha, \\lambda \\xmin)} \\ x_i^{-\\alpha} e^{-\\lambda x_i} 1(x_i \u0026gt; \\xmin) + 1(x_i \\le \\xmin) \\right] f_{\\alpha}(a) \\ f_{\\xmin}(x) $$ where, \\( f_{\\alpha}(a) \\equiv \\text{U}(2,3) \\) and \\( f_{\\xmin}(x) \\propto e^{-10x} \\ 1(x\u0026gt;1) 1(x\u0026lt;3) \\)\nUnknown \\( \\alpha \\), \\( \\lambda \\), and \\( \\xmin \\): In this case, since all three parameters are to be estimated, we use informative priors on each of the parameters. In this case, we use exponential distributions with upper and lower-bound cutoffs on each of them. $$ p(\\boldphi \\ | \\ \\mathcal{D}) \\propto_{\\boldphi} \\prod_{i = 1}^n \\left[ \\frac{\\lambda^{1-\\alpha}} {\\Gamma(1-\\alpha, \\lambda \\xmin)} \\ x_i^{-\\alpha} e^{-\\lambda x_i} 1(x_i \u0026gt; \\xmin) + 1(x_i \\le \\xmin) \\right] f_{\\alpha}(a) \\ f_{\\lambda}(l) \\ f_{\\xmin}(x) $$ where, \\( f_{\\alpha}(a) \\propto e^{-10a} \\ 1(a\u0026gt;2) 1(a\u0026lt;3) \\), \\( f_{\\lambda}(l) \\propto e^{-10l} \\ 1(l\u0026gt;0) 1(l\u0026lt;3) \\) and \\( f_{\\xmin}(x) \\propto e^{-10x} \\ 1(x\u0026gt;1) 1(x\u0026lt;3) \\)\nSimulation Simulation involves the generation of power-law samples with exponential cutoffs, with true parameters, and the estimation of these parameters. The distribution is complex to sample from using traditional methods like inverse-transform sampling, and hence, we have to use Monte Carlo methods to draw samples. None of our priors are conjugate priors, and we do not have a close-form solution for the posterior. Therefore, to perform parameter estimation, we would need to use Monte Carlo methods.\nGeneration of the power-law samples with exponential cutoff Since direct sampling is difficult, we can use Accept-Reject sampling to sample from the distribution. For Accept-Reject sampling, we need to use a covering distribution \\( g(x) \\) for which \\( M g(x) \\ge l(x) \\) where \\( M \\) is a scalar, and \\( l(x) = c \\pi(x) \\) where \\( \\pi(x) \\) is the power-law distribution with exponential cutoff. We can use a traditional power-law distribution (no exponential cut-off) as a covering distribution, as the density of the exponential cutoff declines faster than the traditional power-law distribution.\nThe probability density function of a traditional power-law distribution is given by,\n$$ g(x\\ | \\ x_m,\\alpha) = \\frac{\\alpha-1}{x_m} \\left(\\frac{x}{x_m}\\right)^{-\\alpha} \\ , \\ x\u0026gt;x_m $$\nNow we use inverse transform sampling to draw samples from the traditional power-law distribution using uniform random samples.\n$$ \\begin{aligned} G(x) \u0026= \\int_{-\\infty}^x g(x\\ | \\ x_m,\\alpha) \\ dx\\\\ \u0026= \\int_{-\\infty}^x \\frac{\\alpha-1}{x_m} \\left(\\frac{x}{x_m}\\right)^{-\\alpha} I(x\u003ex_m) \\ dx \\\\ \u0026= \\int_{x_m}^x \\frac{\\alpha-1}{x_m} \\left(\\frac{x}{x_m}\\right)^{-\\alpha} \\ dx \\\\ G(x) \u0026= 1- \\left( \\frac{x}{x_m} \\right)^{-\\alpha+1}\\\\ u \u0026= 1- \\left( \\frac{x}{\\xmin} \\right)^{-\\alpha+1}\\\\ x \u0026= \\xmin(1-u)^{\\frac{1}{1-\\alpha}} \\end{aligned} $$ Here, \\( x \\) is a sample from the distribution \\( g(x|\\alpha, \\lambda, \\xmin) \\). It can be shown that,\n$$ M = \\frac{\\lambda^{1-\\alpha} \\ e^{-\\lambda \\xmin} \\ \\xmin} {(\\alpha-1) \\ \\Gamma(1-\\alpha, \\lambda \\xmin) \\ \\xmin^{\\alpha}} $$ where, \\( M \\) satisfies the condition, \\( M g(x) \\ge l(x) \\)\nAccept-Reject algorithm Draw \\( x \\) from \\( g() \\) Compute \\( \\) r = \\frac{l(x)}{M g(x)} \\ (\\le 1) \\( \\) Draw \\( f=\\text{Bernoulli}(r) \\) if \\( f = 1 \\), accept and return \\( x \\) if \\( f = 0 \\), reject \\( x \\) Accept-Rejection sampling to generate Power Law samples with Exponential Cutoff in R Import the necessary libraries and set a seed\nset.seed(84884) library(expint) library(remotes) library(sigmoid) Set the true-parameters\nalpha \u0026lt;- 2.2 lambda \u0026lt;- 0.3 xmin \u0026lt;- 1.1 theta_true \u0026lt;- c(alpha, lambda, xmin) c \u0026lt;- lambda^(1-alpha)/(gammainc(1-alpha, lambda*xmin)) Write a function for the PDF of the power-law distribution with an exponential cutoff\nfp \u0026lt;- function(x){ if (x \u0026gt; xmin){ return(c * ((x^ (-1 * alpha)) * (exp(-1 * lambda * x)))) } else { return(0) } } Write functions to sample from the covering distribution (vanilla power-law)\nfoo \u0026lt;- function(xmin, alpha){ u \u0026lt;- runif(1,0,1) return(xmin*((1-u)^(1/(1-alpha)))) } generate \u0026lt;- function(n, xmin, alpha){ return(as.numeric(replicate(n, foo(xmin, alpha), simplify=FALSE))) } getCumulative \u0026lt;- function(x, xmin, alpha){ return(1-((x/xmin)^(1-alpha))) } covering \u0026lt;- function(i){ return(generate(1, xmin, alpha)) } Write a function to obtain the PDF of the covering distribution\nc_c \u0026lt;- ((alpha-1)/xmin) * (xmin^alpha) covering_pdf \u0026lt;- function(x){ if (x \u0026gt; xmin){ return(((alpha-1)/xmin)*((x/xmin)^(-alpha))) } else{ return(0) } } M \u0026lt;- c*(exp(-lambda*(xmin)))/c_c I usually like to write a helper function to calculate the Accept-Reject ratio\ncompute_ratio \u0026lt;- function(u){ return(fp(u)/(M*covering_pdf(u))) } Perform Accept-Reject sampling\nsamples \u0026lt;- list() for (i in 1:500){ sample \u0026lt;- covering() r \u0026lt;- compute_ratio(sample) result \u0026lt;- rbinom(1,1,prob=r) if (result == 1){ samples \u0026lt;- c(samples, sample) } } samplesPL \u0026lt;- as.numeric(samples) length(samplesPL) We can now plot the density of the power-law samples obtained from the Accept-Reject sampling method\nplot(density(samplesPL), xlab = \u0026#34;x\u0026#34;, ylab = \u0026#34;Frequency\u0026#34;, main=\u0026#34;Density\u0026#34;) Density of samples obtained from accept-reject sampling\nBayesian inference on the parameters Bayesian inference using Monte Carlo methods involves sampling from the posterior to find the parameter that gives the highest value on the posterior (mode of the posterior distribution). Since in case 2 and case 3, we need to estimate more than one parameter, our sampling from the posterior involves drawing random vectors from the posterior. In this case, we use the Metropolis-Hasting algorithm for drawing samples from the posterior. Given a proposal distribution \\( q(\\boldsymbol{\\phi_i} | \\boldsymbol{\\phi_{i-1}}) \\) and the likelihood function \\( \\mathcal{L}(\\mathcal{D}, \\boldphi) \\).\nStart with \\( \\boldsymbol{\\phi_0} \\) For \\( i=1, 2, \\dots n \\): Draw \\( \\boldsymbol{\\phi^*} \\) from \\( q(\\boldsymbol{\\phi_{i-1}}) \\) Compute \\( a = \\frac{\\mathcal{L}(\\mathcal{D}, \\boldsymbol{\\phi^*}) q(\\boldsymbol{\\phi_{i-1}} | \\boldsymbol{\\phi^*})}{\\mathcal{L}(\\mathcal{D}, \\boldsymbol{\\phi_{i-1}}) q(\\boldsymbol{\\phi^*} | \\boldsymbol{\\phi_{i-1}})} \\) if \\( a \u0026gt; 1 \\), accept \\( \\boldsymbol{\\phi^*} \\), \\( \\boldsymbol{\\phi_i} = \\boldsymbol{\\phi^*} \\) if \\( 0 \u0026lt; a \u0026lt; 1 \\), accept \\( \\boldsymbol{\\phi^*} \\), \\( \\boldsymbol{\\phi_i} = \\boldsymbol{\\phi^*} \\) with probability \\( a \\) Our proposal distributions, like prior distributions, vary with each case.\nCase 1: $$ q(x | \\boldsymbol{\\phi_{i-1}}) = \\boldsymbol{\\phi_{i-1}} e^{\\boldsymbol{\\phi_{i-1}} \\ x} \\ 1(1\u0026gt;0) $$ Case 2: $$ \\boldsymbol{q}(\\boldsymbol{x} | \\boldsymbol{\\phi_{i-1}}) = \\begin{bmatrix} \\boldsymbol{\\phi_{i-1}}^{[1]} e^{\\boldsymbol{\\phi_{i-1}}^{[1]} \\ x_1} \\ 1(x_1\u0026gt;0) \\\\ \\boldsymbol{\\phi_{i-1}}^{[2]} e^{\\boldsymbol{\\phi_{i-1}}^{[2]} \\ x_2} \\ 1(x_2\u0026gt;0) \\\\ \\end{bmatrix} $$ Case 3: $$ \\boldsymbol{q}(\\boldsymbol{x} | \\boldsymbol{\\phi_{i-1}})= \\begin{bmatrix} \\boldsymbol{\\phi_{i-1}}^{[1]} e^{\\boldsymbol{\\phi_{i-1}}^{[1]} \\ x_1} \\ 1(x_1\u0026gt;0) \\\\ \\boldsymbol{\\phi_{i-1}}^{[2]} e^{\\boldsymbol{\\phi_{i-1}}^{[2]} \\ x_2} \\ 1(x_2\u0026gt;0) \\\\ \\boldsymbol{\\phi_{i-1}}^{[3]} e^{\\boldsymbol{\\phi_{i-1}}^{[3]} \\ x_3} \\ 1(x_3\u0026gt;0)\\\\ \\end{bmatrix} $$ Case 1 :\n\\( \\alpha \\) - Jeffrey\u0026rsquo;s prior \\( \\lambda \\) - known \\( \\xmin \\) - known lambda \u0026lt;- theta_true[2] xmin \u0026lt;- theta_true[3] Gamma function\nG \u0026lt;- function(alpha){ suppressWarnings(gammainc(1-alpha, lambda*xmin)) } The first derivative of the Gamma function\nGp1 \u0026lt;- function(alpha){ -1 * alpha * suppressWarnings(gammainc(-1*alpha, lambda*xmin)) } The second derivative of the Gamma function\nGp2 \u0026lt;- function(alpha){ (alpha*(alpha+1)*suppressWarnings(gammainc(-1-alpha, lambda*xmin))) - suppressWarnings(gammainc(-1*alpha, lambda*xmin)) } prior_alpha \u0026lt;- function(alpha){ j \u0026lt;- ((G(alpha)*Gp2(alpha))-(Gp1(alpha)^2))/(G(alpha)^2) if (is.na(j)){ return(0) } if (j \u0026lt; 0){ j \u0026lt;- j*-1 } else{ return(sqrt(j)) } } pllikelihood \u0026lt;- function(x, alpha){ if (x \u0026gt; xmin){ return(((x^(-alpha)) * exp(-lambda * x) * (lambda^(1 - alpha)))/suppressWarnings(gammainc(1-alpha, lambda * xmin))) } else { return(1) } } loglikelihood \u0026lt;- function(data, alpha){ lik \u0026lt;- lapply(data, pllikelihood, alpha=alpha) blee \u0026lt;- lapply(lik, log) return(sum(as.numeric(blee))) } Posterior distribution known up to a constant\nlogpostfunc \u0026lt;- function(data, alpha){ alpha \u0026lt;- as.numeric(alpha) return(loglikelihood(data, alpha)+log(prior_alpha(alpha))) } Proposal distribution\nproposal \u0026lt;- function(given){ k \u0026lt;- as.numeric(given) return(rexp(1, k)) } proposal_pdf \u0026lt;- function(this, given){ return(dexp(as.numeric(this), as.numeric(given))) } Metropolis-Hasting for Bayesian Inference\nreps \u0026lt;- 100 acc \u0026lt;- list() params \u0026lt;- list() pb = txtProgressBar(min = 1, max = reps, initial = 1) for (v in 1:reps){ nit \u0026lt;- 100 accepted \u0026lt;- 0 theta \u0026lt;- list() theta[1] \u0026lt;- proposal(2.1) for(i in 2:nit){ theta_star \u0026lt;- proposal(theta[i-1]) a = (logpostfunc(samplesPL, theta_star) + log(proposal_pdf(theta[i-1], theta_star)))-logpostfunc(samplesPL,theta[i-1]) - log(proposal_pdf(theta_star,theta[i-1])) a \u0026lt;- exp(a) if (is.nan(a)){ theta[i] \u0026lt;- theta[i-1] } else if (a \u0026gt; 1){ theta[i] \u0026lt;- theta_star accepted \u0026lt;- accepted+1 } else{ r = rbinom(1,1,a) if (r == 1){ theta[i] \u0026lt;- theta_star accepted \u0026lt;- accepted+1 } else{ theta[i] \u0026lt;- theta[i-1] } } } acc[v] \u0026lt;- accepted/nit post_pdf \u0026lt;- lapply(theta, logpostfunc, data=samplesPL) theta_selected \u0026lt;- theta[which.max(post_pdf)] params[v] \u0026lt;- theta_selected setTxtProgressBar(pb,v) } params \u0026lt;- as.numeric(params) plot(d, xlab = \u0026#34;alpha\u0026#34;, ylab = \u0026#34;Density\u0026#34;, main = \u0026#34;Density of alpha on the posterior\u0026#34;) abline(v=d$x[i], col=\u0026#34;blue\u0026#34;) abline(v=alpha, col=\u0026#34;red\u0026#34;) mean(params) The MCMC algorithm implementation for the rest of the cases is a bit more complex as it involves random walks in higher dimensions. It can be found on my GitHub page.\nResults The simulation discussed above is implemented in R. For each case, the same samples are used, which are drawn from the distribution with the true parameters using Accept-Reject sampling. We generate 355 samples from the distribution with the true parameters, \\( \\alpha=2.2 \\), \\( \\lambda=0.3 \\), and \\( \\xmin=1.1 \\), and use the same for inference in the three cases discussed above. In each of the three cases, the Metropolis-Hasting algorithm was used to draw 1000 samples \\( {\\boldsymbol{\\phi}^{[1]}, \\boldsymbol{\\phi}^{[2]} \\dots \\boldsymbol{\\phi}^{[1000]}} \\) from the posterior, and \\( \\hat{\\boldsymbol{\\phi}} = \\text{argmax}_{\\boldsymbol{\\phi}} \\ p(\\boldphi \\ | \\ \\mathcal{D}) \\) is calculated, where, \\( \\mathcal{D} \\) is the sample generated from the distribution with known true parameters. This is repeated 100 times to calculate the estimate (\\( \\hat{\\boldsymbol{\\phi}} \\)) at each iteration.\nAs we can see from Fig (1-6), we have the set of estimates for the parameters in each case, which are very close to the true value used during the simulation. When priors are more informative (case 2 and case 3), and when the number of estimable parameters is lower (case 1 and 2) we have better estimates of the parameters.\nCase Parameter True Estimated 1 \\( \\alpha \\) 2.2 2.05 2 \\( \\alpha \\) 2.2 2.19 2 \\( \\xmin \\) 1.1 1.38 3 \\( \\alpha \\) 2.2 2.51 3 \\( \\lambda \\) 0.3 0.29 3 \\( \\xmin \\) 1.1 1.29 Table 1: Results of the estimation Density plot for \\hat{\\alpha} after 100 iterations in Case 1 Density plot for $\\hat{\\alpha}$ after 100 iterations in Case 2 Density plot for $\\hat{\\xmin}$ after 100 iterations in Case 2 Density plot for $\\hat{\\xmin}$ after 100 iterations in Case 3 Density plot for $\\hat{\\xmin}$ after 100 iterations in Case 3 Density plot for $\\hat{\\lambda}$ after 100 iterations in Case 3 Density plot for $\\hat{\\xmin}$ after 100 iterations in Case 3 Conclusion In this research, we used MCMC for Bayesian parameter estimation from a power-law distribution with an exponential cutoff. First, we implemented an Accept-Reject method to sample from the distribution for later inference. Then, we derived Jeffrey\u0026rsquo;s prior for one of the cases and used the Metropolis-Hasting algorithm to sample from the prior. For the proposal function, we derived the inverse transform sampling function for the traditional power-law distribution. Parameters were then estimated from the posterior samples by finding the mode.\nIn disciplines where we encounter power-law distributions in empirical data, having prior knowledge of the parameters and their bounds can significantly improve Bayesian estimations as observed. Even though this method can be used for estimation, for case 3, efficiency was as low as 15%. For inference on higher parameter dimensions, Hamiltonian Monte-Carlo can be used to yield higher efficiency.\nReferences 1. A. Clauset, C. R. Shalizi, and M. E. J. Newman, “Power-Law Distributions in Empirical Data,” SIAM Review, vol. 51, no. 4. Society for Industrial \u0026amp; Applied Mathematics (SIAM), pp. 661–703, Nov. 04, 2009. doi: 10.1137/070710111 2. Z. Zhu and R. A. Marcus, “A Maximum Likelihood Method for Power-law Distributions That Does Not Break Down When the Slope Is Close to Unity,” The Journal of Physical Chemistry C, vol. 116, no. 27. American Chemical Society (ACS), pp. 14690–14693, Jun. 20, 2012. doi: 10.1021/jp303697j 3. F. Olmez, P. R. Kramer, J. Fricks, D. R. Schmidt, and J. Best, “Penalized KS method to fit data sets with power-law distribution over a bounded subinterval,” Journal of Statistical Computation and Simulation, vol. 91, no. 8. Informa UK Limited, pp. 1524–1563, Jan. 14, 2021. doi: 10.1080/00949655.2020.1861281 4. B. Nettasinghe and V. Krishnamurthy, “Maximum Likelihood Estimation of Power-law Degree Distributions via Friendship Paradox-based Sampling,” ACM Transactions on Knowledge Discovery from Data, vol. 15, no. 6. Association for Computing Machinery (ACM), pp. 1–28, May 19, 2021. doi: 10.1145/3451166 5. Harney, Hanns L. Bayesian inference : parameter estimation and decisions. Berlin New York: Springer, 2003 6. F. Pennini and A. Plastino, “Power-law distributions and Fisher’s information measure,” Physica A: Statistical Mechanics and its Applications, vol. 334, no. 1–2. Elsevier BV, pp. 132–138, Mar. 2004. doi: 10.1016/j.physa.2003.10.076 7. Liu, Jun S. Monte Carlo strategies in scientific computing. New York: Springer, 2008 8. Casella, George, and Roger L. Berger. Statistical inference. Australia Pacific Grove, CA: Thomson Learning, 2002 9. Michael I. Jordan Jeffreys Priors and Reference Priors. Berkeley, CA: Bayesian Modeling and Inference February 17, 2010 10. Michael Betancourt: “A Conceptual Introduction to Hamiltonian Monte Carlo”, 2017; [http://arxiv.org/abs/1701.02434 arXiv:1701.02434 ","permalink":"http://localhost:1313/stats/mcmc_powerlaw/","summary":"$$ \\gdef{\\boldphi}{\\boldsymbol{\\phi}} \\gdef{\\xmin} {x_{\\text{min}}} \\gdef{\\gammaone} {\\Gamma_{\\alpha}^{\\prime} (1-\\alpha, \\lambda \\xmin)} \\gdef{\\gammatwo}{\\Gamma_{\\alpha}^{\\prime \\prime} (1-\\alpha, \\lambda \\xmin)} \\gdef{\\gammazero}{\\Gamma (1-\\alpha, \\lambda \\xmin)} $$ Introduction The power-law distribution is of the form \\( f(x) \\propto x^{-\\alpha} \\), where \\( \\alpha \\) is called the scaling parameter. It models many natural phenomena like acoustic attenuation, Curie–Von Schweidler law, neuronal avalanches, and others. As \\( x \\to 0 \\), \\( f(x) \\) diverges out of bounds. Hence, we define \\( \\xmin \\) as a lower bound for the support of the distribution function \\( f(x) \\).","title":"Bayesian Estimates for the Parameters of a Power-law Distribution with Exponential Cutoff using Monte Carlo Methods"},{"content":" This is not really stats project but it is pretty cool $$ \\gdef{\\boldphi}{\\boldsymbol{\\phi}} $$\nThe Dirichlet Function The Dirichlet function is one of the easiest functions to define. It\u0026rsquo;s literally,\n$$ \\begin{aligned} f(x)\u0026=\\left\\{ \\begin{array}{ll} 1, \u0026 \\text{if } x \\text{ is rational} \\\\ 0, \u0026 \\text{if } x \\text{ is not rational} \\end{array} \\right. \\end{aligned} $$ or using the symbols from number theory/set theory that almost no one remembers,\n$$ \\begin{aligned} f(x)=\\left\\{ \\begin{array}{ll} 1, \u0026amp; \\text{if $x \\in \\mathbb{R -Q}$}\\\\ 0, \u0026amp; \\text{if $x \\in \\mathbb{Q}$} \\end{array} \\right. \\end{aligned} $$ While this seems simple to define, things get more and more complex as you try to explore the behavior of the function. For one, this is a highly discontinuous function with range \\( [0, 1] \\) and domain \\( \\mathbb{R} \\). This is a very strange bounded non-monotonic function that is \u0026ldquo;nowhere continuous\u0026rdquo;, and not just infinitely discontinuous.\nLet\u0026rsquo;s learn the definitions of rational numbers, irrational numbers, and integrability, before we try to see if this function can be integrated.\nRational and Irrational Numbers Definitions A rational number is any number that can be expressed as a ratio of two integers.\nMathematically,\n$$ \\begin{aligned} \u0026amp;p/q, \u0026amp; p, q \\in \\mathbb{Z}, q \\ne 0 \\end{aligned} $$ An irrational number is any real number that isn\u0026rsquo;t a rational number. In other words, irrational numbers can not be expressed as a ratio of two integers. The examples include \\( \\pi \\), \\( e \\), \\( \\sqrt{2} \\), etc.\nHistory The studies pertaining to rational and irrational numbers date back to Ancient Greece where one of the Philosophers tried to solve for the hypotenuse of the right angle triangle from the Pythagoras theorem, only to end up with some messy numbers with contradictions (in this era, the irrational numbers were called \u0026lsquo;immeasurable quantities\u0026quot;). In 800 BC, ancient Indians said, \u0026ldquo;This is unsolvable! we are going to use numerical methods to find the approximate values for the square roots of unpleasant numbers\u0026rdquo;, and did so in the book Sulabha Sutra. Aryabhata after many years solved the value of pi to a few decimals, in the decimal system. After this, mathematicians in the middle east called measurable quantities and immeasurable quantities \u0026lsquo;real numbers\u0026rsquo; owing to the development of Algebra by notable mathematicians like Al-Khawarizmi. Of course, by the 17th century, we knew enough mathematics from number theory, real analysis, logarithms, which made it easier to analyze irrational numbers.\nA very weird property of the density of Rational and Irrational numbers According to the \u0026ldquo;density\u0026rdquo; property of the rational and irrational numbers, in an interval, \\( (a, b) \\), where \\( a \u0026lt; b \\), there exists a \\( c \\in (a,b) \\) such that, \\( c \\in \\mathbb{Q} \\) and a \\( d \\in (a,b) \\) such that, \\( d \\in \\mathbb{R - Q} \\).\nNo matter how small the interval is (\u0026lsquo;a\u0026rsquo; in mathematics often means \u0026lsquo;at least a\u0026rsquo; or \u0026lsquo;one or more than one\u0026rsquo;). Imagine that!. Between 3 and 5, there\u0026rsquo;s a rational number and an irrational number. Between 0.1 and 0.2, there\u0026rsquo;s a rational number and an irrational number. Between 0.000000001 and 0.000000002, there\u0026rsquo;s a rational number and an irrational number. You get the point. Keep this point in mind. We will be using it later on.\nIntegrability The integral that we\u0026rsquo;ve studied in calculus in school is called the Riemann integral. If the integral of a function is an operation that calculates the area as the function of the support, we\u0026rsquo;d definitely want the function to be infinite in that interval, as the sum of infinity with any other finite number is infinity which is not defined. We would definitely not want the function to jump around so much that no finite interval can be found on the x-axis to find its area. With the Dirichlet function, we seem to have that problem. But let\u0026rsquo;s mathematically try to see if it is so.\nAccording to the Riemann-Darboux integrability criterion, you can find the integral of a bounded function in an interval \\( [a, b] \\) only if it satisfies the following.\nIf a partition of \\( [a, b] \\) is a finite sequence, such that \\( a \u0026lt; x_1 \u0026lt; x_2 \u0026lt; \\dots \u0026lt; x_{n-1} \u0026lt;b \\), consider the boundary points set \\( P = \\lBrace x_1, x_2, \\dots , x_{n-1} \\rBrace \\)\nNow define,\n$$ \\begin{aligned} M_i = \\displaystyle{\\sup_{x \\in [x_{i-1}, x_i]}}f(x) \\\\ \\\\ m_i = \\displaystyle{\\inf_{x \\in [x_{i-1}, x_i]}}f(x) \\end{aligned} $$ If this confuses you, just know that $M_i$ is a function that finds the maximum value of \\( f(x) \\) in the interval \\( [x_{i-1}, x_i] \\), and \\( m_i \\) finds the minimum value of \\( f(x) \\) in the interval \\( [x_{i-1}, x_i] \\).\nNow if \\( a = x_0 \\) and \\( b = x_n \\), then the Upper Darboux Sum is given by,\n\\( U_{f,P} = \\sum_{i=1}^n(x_i-x_{i-1}) \\ M_i \\) and the Lower Darboux Sum is given by, \\( L_{f,P} = \\sum_{i=1}^n(x_i-x_{i-1}) \\ m_i \\)\nThen the integral of \\( f(x) \\) in the interval \\( [a, b] \\) is given by,\n$$ \\begin{aligned} \\int_a^b f(x) \\ dx \u0026= \\inf\\{{U_{f,P}:P \\ \\text{is any partition of }[a, b]\\}} \\\\ \u0026= \\sup{\\{L_{f,P}:P \\ \\text{is any partition of }[a, b]\\}} \\end{aligned} $$ In other words, we need \\( \\inf{U_{f,P}} = \\sup{L_{f,P}} \\) to be true for the function to be integrable.\nStitching the pieces together Integrability of the Dirichlet function For a partition of \\( [a, b] \\), and the boundary points set \\( P = {x_1, x_2, \\dots, x_{n-1} } \\), let\u0026rsquo;s define -\n$$ \\begin{aligned} \u0026amp;M_i \u0026amp;= \\displaystyle{\\sup_{x \\in [x_{i-1}, x_i]}}f(x) \\\\ \\\\ \u0026amp;m_i \u0026amp;= \\displaystyle{\\inf_{x \\in [x_{i-1}, x_i]}}f(x) \\end{aligned} $$ where, just to recap, the function at hand is,\n$$ \\begin{aligned} f(x)=\\left\\{ \\begin{array}{ll} 1, \u0026amp; \\text{if $x \\in \\mathbb{R -Q}$}\\\\ 0, \u0026amp; \\text{if $x \\in \\mathbb{Q}$} \\end{array} \\right. \\end{aligned} $$ From the \u0026ldquo;density property\u0026rdquo;, we know that in every interval \\( [x_{i-1}, x_i] \\), there\u0026rsquo;s a rational number and there\u0026rsquo;s an irrational number. And furthermore, for a rational number, \\( f(x) = 1 \\) and for an irrational number \\( f(x)=0 \\). And also, we know that \\( 0\u0026lt;1 \\) (lol).\nTherefore, \\( M_i = \\displaystyle{\\sup_{x \\in [x_{i-1}, x_i]}}f(x) = 1 \\) in every interval of \\( P \\).\nSimilarly, \\( m_i = \\displaystyle{\\inf_{x \\in [x_{i-1}, x_i]}}f(x) = 0 \\) in every interval of \\( P \\).\nHence, the lower and upper Darboux sums become,\n$$ \\begin{aligned} \u0026amp;U_{f,P} = \\sum_{i=1}^n(x_i-x_{i-1}) \\ M_i = \\sum_{i=1}^n(x_i-x_{i-1}) \\ (1) = (x_n-x_0) \\ (1) = b-a \\\\ \u0026amp;L_{f,P} = \\sum_{i=1}^n(x_i-x_{i-1}) \\ m_i = \\sum_{i=1}^n(x_i-x_{i-1}) \\ (0) = 0 \\end{aligned} $$ This is true irrespective of the choice of the partition \\( P \\).\nThis means that, \\( \\inf{U_{f,P}} = U_{f,P} = b-a \\) and \\( \\sup{L_{f,P}} = L_{f,P} = 0 \\) This means that, \\( \\inf{U_{f,P}} \\ne \\sup{L_{f,P}} \\), which means that the Dirichlet function is not integrable.\n","permalink":"http://localhost:1313/stats/dirichlet_distribution/","summary":"This is not really stats project but it is pretty cool $$ \\gdef{\\boldphi}{\\boldsymbol{\\phi}} $$\nThe Dirichlet Function The Dirichlet function is one of the easiest functions to define. It\u0026rsquo;s literally,\n$$ \\begin{aligned} f(x)\u0026=\\left\\{ \\begin{array}{ll} 1, \u0026 \\text{if } x \\text{ is rational} \\\\ 0, \u0026 \\text{if } x \\text{ is not rational} \\end{array} \\right. \\end{aligned} $$ or using the symbols from number theory/set theory that almost no one remembers,","title":"Is the Dirichlet Function Riemann-Darboux Integrable?"},{"content":" This project was co-authored by Heather Johnston and Youngwoo Kwon $$ \\gdef{\\boldphi}{\\boldsymbol{\\phi}} $$\nIntroduction COVID-19, or Coronavirus Disease 2019, was declared a pandemic by the World Health Organization on March 11, 2020. The disease originated in December 2019 in China and quickly spread around the world. Compared with the previous epidemics like Ebola and SARS, COVID-19 has a relatively low mortality rate, but spreads faster with numerous mildly symptomatic cases. In response to the spread of COVID-19, governments and institutions have implemented a variety of policies such as social distancing, masks requirements, stay-at-home orders, and travel restrictions to limit or prevent person-to-person interactions.\nOur project explores the distribution of case numbers of COVID-19 in the United States over the past two years and how the infection and recovery rate have been changed during the pandemic. To determine how the shift between susceptible – inspection – recovery groups has occurred for coronavirus, our project adapts the conventional SIR model to estimate the infection and recovery rates using data reported at the daily level. The present project applies Monte Carlo Markov Chain to find the proposal distributions for infection rate and recovery rate, and the Metropolis-Hasting algorithm with prior distributions and proposal distributions to estimate the posterior distribution of infection rate and recovery rate. Eventually, the project forecasts the future pandemic data using infection rate and recovery rate from the previous data.\nData The data for our project comes from the Oxford Government Responses Tracker, and the Our World in Data COVID-19 repository. The data from the Our World in Data Covid-19 data set includes infomration for each country each day with number of new cases and total population. The data from the Oxford Government Responses Tracker includes vaccination and facial covering policy for each location beginning in December of 2020, with results reported for users throughout much of the world.\nThe infection rate is an indicator of how probable it is that a susceptible person (ie a person who has never before been infected) will become infected in a given time period, which is subject to various factors such as government policy, the degree of infections of the virus in the country, or the degree of the mutation of the virus. To establish more accurate infection rates, our project filtered the data by choosing the United States as the location of interest. Our project set vaccination rate, mask policies, and degree of the mutation of the virus as the three main predictors with which to slice the data, allowing us to fit separate infection rates for each time period.\nFigure 1. Fully vaccinated rate in United States from “2020-01-01” to “2022-04-10” Figure 1 shows how the number of people fully vaccinated changed over time. A population with a higher fully-vaccinated percentage has a stronger resistance to viruses across the population, which results in a decrease in the infection rate. To reflect the strong relationship between the fully vaccinated rate and the infection rate, our project divided the data into two groups based on the time when the fully vacated rate reached , which was July 13, 2021.\nFigure 2. Facial Coverings policy in United States from “2020-01-01” to “2022-04-10” Figure 2 shows how the mask policy in United States changed over time. A strong mask restriction policy can reduce the rate of virus transmission between people and consequently produce a low infection rate. Our project divided the data into three groups based on the time when the mask policy in United States have been changed from an average of 2.5 to 3, which is “2020-10-19” and “2021-05-23”.\nOn 26 November 2021, WHO designated a new variant of COVID-19, named Omicron. Compared to the original COVID-19 virus, Omicron spreads more quickly. Omicron resulted in a huge surge in cases in the pandemic around the world. Thus, our project divided the data into two parts, before and after the omicron, to reflect this impact, which is “2021-11-01”.\nAs a result, our research divided the time interval into five,\nBefore\n“2020-10-19” From “2020-10-18” to “2021-05-23” From “2021-05-22” to “2021-07-13” From “2021-07-12” to “2021-11-01” From “2021-10-31” to “2022-04-10” Methods As discussed before, we use the SIR model to model the epidemic stochastic process. Even though the epidemic process is a continuous time stochastic process, we only observe the aggregated data for example: cases per day, cases per week, etc. Therefore, we need to make necessary adjustments to the model to describe the data.\nModel For the SIR model, we use three random variables to describe the epidemic process, \\( S(t) \\), \\( I(t) \\), and \\( R(t) \\) where \\( S(t) \\) is the number of infected people, \\( I(t) \\) is the number of infected people, and \\( R(t) \\) is the number of recovered people at time \\( t \\). We can see that the random variables satisfy\n$$ \\begin{aligned} N \u0026= S(t) + I(t) + R(t) \\newline \\\\ \\frac{dN}{dt} \u0026= \\frac{dS(t)}{dt} + \\frac{dI(t)}{dt} + \\frac{dR(t)}{dt} \\end{aligned} $$ where \\( N \\) is the population of the system.\nBetween any two consecutive observations at \\( t_1 \\) and \\( t_2 \\) where \\( t_2-t_1 = \\Delta t \\), only one of the following three transitions can occur, \\( (s, i) \\xrightarrow{\\Delta t} (s-1, i+1) \\) corresponding to an infection event ( \\(e_i \\) ), \\( (s, i) \\xrightarrow{\\Delta t} (s, i-1)\\) corresponding to a recovery event ( \\(e_r\\) ), or \\( (s, i) \\xrightarrow{\\Delta t} (s, i) \\) corresponding to a null event ( \\( e_n \\) ). We can also write out the transition probabilities corresponding to the three events, where \\( s \\) is the number of susceptible people and \\( i \\) is the number of infected people in the population. It can be shown that,\n$$ \\begin{aligned} p_{e_i}(\\Delta t) \u0026amp;= \\frac{\\beta s i}{N} \\Delta t \\newline \\\\ p_{e_r}(\\Delta t) \u0026amp;= \\gamma i \\Delta t \\end{aligned} $$\nwhere \\( \\beta \\) is the infection rate, and \\( \\gamma \\) is the recovery rate.\nTherefore, the transition probabilities can be summarized as,\n$$ \\begin{aligned} p(\\Delta t) \u0026amp;= \\begin{cases} \\frac{\\beta s i}{N} \\Delta t, \u0026amp; \\text{if} \\ (s, i) \\xrightarrow{\\Delta t} (s-1, i+1) \\newline \\\\ \\gamma i \\Delta t, \u0026amp; \\text{if} \\ (s, i) \\xrightarrow{\\Delta t} (s, i-1) \\newline \\\\ 1 - \\left[\\frac{\\beta s i}{N} \\Delta t + \\gamma i \\Delta t \\right], \u0026amp; \\text{if} \\ (s, i) \\xrightarrow{\\Delta t} (s, i) \\end{cases} \\end{aligned} $$\nWe can see that the transition probabilities change with each infection and recovery events which makes it challenging to deal with the daily aggregate epidemic data. To address this problem, we assume that the infection and recovery rates do not vary within each day.\nWe can now model the SIR infection process as,\n$$ \\begin{aligned} i_d \u0026amp;\\sim \\text{Poisson} \\left( \\frac{\\beta S(t_d) I(t_d)}{N} (t_d - t_{d-1}) \\right) \\newline \\\\ r_d \u0026amp;\\sim \\text{Poisson} \\left( \\gamma i (t_d - t_{d-1}) \\right) \\end{aligned} $$\nThe Bayesian framework The Bayesian framework for the stochastic model can be written as follows,\n$$ p(\\phi \\ | \\ \\mathcal{D}) \\propto_{\\phi} \\mathcal{L}(\\mathcal{D}, \\phi) \\ p(\\phi) $$\nwhere,\n$$ \\mathcal{L}(\\mathcal{D}, \\phi) = \\prod_{d \\in D} p_{e_i}(i_d|i_{d-1}, \\phi) $$\nNow, the Bayesian framework for the estimation of the infection rate \\( \\beta \\) can be written as,\n$$ p(\\beta \\mid \\mathcal{D}) \\propto_{\\beta} \\prod_{d \\in D} p_{e_i}(i_d|i_{d-1}, \\phi) \\ p(\\beta) $$\n$$ p(\\beta \\mid \\mathcal{D}) \\propto_{\\beta} \\left[ \\prod_{d \\in D} \\frac{\\left(\\frac{\\beta S(t_d) I(t_d) \\Delta t}{N}\\right)^{i_d} \\exp{\\left(- \\frac{\\beta S(t_d) I(t_d) \\Delta t}{N}\\right)}}{i_d} \\right] p(\\beta) $$\nWe let \\( p(\\beta) \\) be a gamma prior as \\( \\beta \\) is always positive. We adjust the parameters such that, the mode of the distribution lies in the range \\( 0.1 \u0026lt; \\beta \u0026lt; 0.5 \\) based on previous studies.\nSimilarly, the Bayesian framework for the estimation of the recovery rate \\( \\gamma \\) can be written as,\n$$ \\begin{aligned} p(\\gamma \\mid \\mathcal{D}) \\propto_{\\gamma} \\prod_{d \\in D} p_{e_i}(i_r|i_{d-1}, \\phi) p(\\gamma) \\end{aligned} $$\n$$ p( \\gamma \\mid \\mathcal{D} ) \\propto_{\\gamma} \\left[ \\prod_{d \\in D} \\dfrac{(\\gamma i \\Delta t)^{i_r} \\exp{(- \\gamma i \\Delta t)}}{i_r} \\right] p(\\gamma) $$\nWe let \\( p(\\gamma) \\) be a gamma prior again with appropriate parameters such that, the mode centers at the inverse of the average recovery period.\nSimulation To implement the Bayesian inference for the infection and rate parameters, we propose a data generative model followed by inference to check the efficacy of the algorithm. We adapated the data simulation method Kypraios et al. The data generation involves the following steps:\nInitialize \\( N \\), \\( \\beta \\), \\( \\gamma \\), \\( s=N-1 \\), \\( i=1 \\), and \\( t=0 \\) while \\( i \u0026gt; 0 \\) do, draw \\( \\tau \\sim \\text{Exp}(\\dfrac{\\lambda}{N} s i + \\gamma i) \\) draw \\( u \\sim U(0,1) \\) if \\( u \u0026lt; \\dfrac{\\dfrac{\\lambda}{N} s i }{\\left(\\dfrac{\\lambda}{N} s i + \\gamma i\\right)} \\), \\( s = s - 1 \\), \\( i = i + 1 \\), infectionFlag=1, removalFlag=0 else, \\( i = i - 1 \\), infectionFlag=0, removalFlag=1 \\( t = t + \\tau \\) Record \\( (s, i) \\), \\( (t) \\), (infectionFlag, removalFlag) end while Set \\( T_0 \\) such that \\( T_0 \\) corresponds to time duration with respect to \\( t \\) in a day, and aggregate the simulated data to contain the following information.\nThe day index \\( d \\) The total number of susceptible people in the population at the beginning of day \\( d \\) The total number of infected people in the population at the beginning of day \\( d \\) The total number of infection events on day \\( d \\) The total number of removal events on day \\( d \\) Metropolis-Hasting algorithm As discussed above, the likelihood function is a modification of the Poisson distribution and the prior distribution is a Gamma distribution. Since the posterior looks complex, we implement a simulation based method to draw inference. We use Metropolis-Hasting algorithm to draw from the posterior distribution.\nThe proposal distributions for \\( \\beta \\) and \\( \\gamma \\) are Log-Normal distributions with the mean centered around the interval \\( [0, 0.5] \\) to steer the proposal sampler in the valid random walk path.\nThe proposal distributions are,\n$$ \\begin{aligned} \\beta_{i} | \\beta_{i-1} \u0026amp;\\sim \\text{lognorm}(ln(0.15)+1, \\text{sigmoid}(1/\\beta_{i-1})) \\\\ \\gamma_{i} | \\gamma_{i-1} \u0026amp;\\sim \\text{lognorm}(ln(0.1)+1, 1+ \\text{sigmoid}(1/\\gamma_{i-1})) \\end{aligned} $$\nStart with \\( \\boldsymbol{\\phi_0} \\) For \\( i=1, 2, \\dots n \\): Draw \\( \\boldsymbol{\\phi^+} \\) from \\( q(\\boldsymbol{\\phi_{i-1}}) \\) Compute \\( a = \\frac{\\mathcal{L}(\\mathcal{D}, \\boldsymbol{\\phi^+}) q(\\boldsymbol{\\phi_{i-1}} | \\boldsymbol{\\phi^+})}{\\mathcal{L}(\\mathcal{D}, \\boldsymbol{\\phi_{i-1}}) q(\\boldsymbol{\\phi^+} | \\boldsymbol{\\phi_{i-1}})} \\) if \\( a \u0026gt; 1 \\), accept \\( \\boldsymbol{\\phi^+} \\), \\( \\boldsymbol{\\phi_i} = \\boldsymbol{\\phi^+}\\) if \\( 0 \u0026lt; a \u0026lt; 1 \\), accept \\( \\boldsymbol{\\phi^+} \\), \\( \\boldsymbol{\\phi_i} = \\boldsymbol{\\phi^+} \\) with probability \\( a \\) Application Utilizing data from the Our World in Data COVID-19 data repository, we consider the number of cases of COVID in the United States [Hannah Ritchie and Roser, 2020].\nSince the pandemic has lasted more than two years, with numerous factors influencing the infection rate, such as social distancing, masking, vaccination, and variants, we choose to break the pandemic into five temporal sub-periods, based upon major events in the pandemic and the emergence of the Omicron variant. Furthermore, for the sake of simplicity, we assume that the only possible progression of states for an individual is from susceptible to infected to recovered. In other words, we assume negligible impact of waning immunity over time, and we assume that vaccinated people can still become infected.\nBecause recovery rates are unobserved (at least at the population level), we estimate recovery data in order to demonstrate the utility of our method. In order to estimate the recovery data, we implement a stochastic process using a log-linear model with parameters \\( \\mu = 1.5 \\) and \\( \\sigma^2 = .25 \\), where each new infection is assigned a value according to the log-linear model, and rounded to the nearest integer which is considered the number of days that person will take to recover. This model was chosen based on the CDC\u0026rsquo;s most recent guidance on infectious periods for COVID-19 [CDC 2020]. We assume that the recovery rate is the same throughout the pandemic. The chart below shows the observed number of new infections with vertical bars indicating the spaces in which we separate the periods to estimate the infection rate parameter.\nFigure 4: Observed numbers of new infections in the U.S. with vertical bars in blue indicating sub-period boundaries The chart above shows our estimated rate of current infections, which depends both on the new infection rate (observed) and the recovery time of each individual (estimated). We also show with vertical lines the 4 dates separating the pandemic into 5 sub-periods for parameter estimation. In order to estimate these parameters, we set prior distributions for and as follows:\n$$ \\begin{aligned} p_0(\\beta) \u0026amp; \\sim \\text{Gamma}(2, 5) \\\\ p_0(\\gamma) \u0026amp; \\sim \\text{Gamma}(20, 95) \\end{aligned} $$\nWe chose these distributions based on values estimated in previous papers for the infection and recovery rates for COVID-19 and earlier respiratory diseases like SARS, which tend to be between 0 and 1 [Wang et al., 2021, Wintachai and Prathom, 2021]. We allowed the proposal distributions to have a low probability of high values to allow for our uncertainty in the rates.\nThe proposal distributions for each new \\( \\beta^+ \\) and \\( \\gamma^+ \\) in the Markov Chain are as follows:\n$$ \\begin{aligned} J_{\\beta}(\\beta^* | \\beta_s) \u0026amp; \\sim \\text{Log-normal}(\\mu = \\log (0.2) + 1, \\sigma = S(1/\\beta_s)) \\\\ J_{\\gamma}(\\gamma^* | \\gamma_s) \u0026amp; \\sim \\text{Log-normal}(\\mu = \\log (0.22) + 1, \\sigma = 1+S(1/\\gamma_s)) \\end{aligned} $$\nWhere \\( S(.) \\) denotes the sigmoid function, so \\( S(x) = [1 + e^{-x}]^{-1} \\).\nForecasting U.S. case numbers Utilizing the estimated infection rate \\( \\beta \\) and recovery rate \\( \\gamma \\) for the United States, we predict the data ranging from April 10, 2022 through April 30, 2022. We are able to assess the accuracy of our forecast for the eight days from April 10 through April 18 using visualization methods and numerical comparisons.\nResults After implementing our Metropolis-Hasting algorithm for estimating the parameters of the simulated infections data, we obtained a stable set of \\( \\beta_s \\) values representing our posterior distribution, with a posterior mean within rounding error of the true values for both parameters. We proceeded to apply our method to the actual U.S. COVID case data.\nMCMC procedure diagnostics When implementing our Metropolis-Hasting algorithm for the real COVID data, we witnessed that our algorithm converged quickly, with values not changing much after the initial accepted \\( \\theta^+ \\). Furthermore, the acceptance rate for \\( \\theta^+ \\) values generated from the proposal distribution was low, which indicates that our algorithm was not as efficient as it might have been.\nU.S. COVID infection and recovery rate After implementing the Metropolis-Hastings procedure with the prior distributions and proposal distributions described in section 4, we got a resulting empirical estimate of the posterior distributions of each \\( \\beta_i \\) corresponding to the $i$th period of the pandemic and the overall recovery rate $\\gamma$. The resulting estimates are shown in the table below.\nParameter Posterior mean \\( \\beta_1 \\) \\( 0.234 \\) \\( \\beta_2 \\) \\( 0.229 \\) \\( \\beta_3 \\) \\( 0.269 \\) \\( \\beta_4 \\) \\( 0.251 \\) \\( \\beta_5 \\) \\( 0.253 \\) \\( \\gamma \\) \\( 0.212 \\) Forecasting future pandemic data By assuming the mean of the posterior distributions for \\( \\gamma \\) and \\( \\beta_5 \\), we implement our data generation process based on the observed infection numbers from April 9, 2022. Recall that the parameter \\( \\beta_5 \\) was estimated for November 1, 2021 through April 9, 2022. We use that to predict the data for the rest of April. Since we can only compare our model to the observed value of new case numbers, we limit our assessment to that metric for April 10 through April 18.\nFigure 5: Predicted numbers of new infections compared to actual number of new infections As seen in the plot above, the true infection data is considerably more variable than our forecasted data, with much lower values associated with weekends. However, if we look at the total infections over the nine days for which we are able to compare our forecast, we see that our model performed well, predicting the total number of infections to within 500 of the observed number.\nData Total Infections Forcasted 285136 Observed 285621 Total Infections for April 10 – 18, 2022 Conclusion Given the massive scale of the COVID-19 pandemic, traditional SIR models which rely on modeling the probability of infection at very small time intervals or continuous time become computationally infeasible quickly. With thousands of new infections per day in the U.S. alone, it would be impossible to use MCMC methods to estimate the parameters for every new infection and recovery. Our method, which assumes a constant infection rate across each day, effectively utilizes a Bayesian framework to estimate the parameters of the model. Our procedure appears to have done a reasonably good job estimating the infection and recovery rate parameters on both simulated and real data. Though it was not as efficient an algorithm as we desired, it was able to predict with considerable accuracy the total infections over 9 days in the U.S.\nOur analysis carries several important limitations. Firstly, researchers are unable to observe recovery times, so knowing the true length of the infectious period for each individual with COVID is impossible, as is verifying the accuracy of predicted recovery counts. Secondly, we are aware that COVID testing and reporting is imperfect, which was true due to a lack of testing early in the pandemic, and is now an issue again due to the common use of home test kits. Thirdly, our model uses highly diffuse prior distributions, which could be improved by consulting with expert epidemiologists. Finally, although we have tried to take into account factors affecting the infection rate such as vaccination rates and mask mandates, our model is likely to still be insufficiently complex to capture the many factors which influence the course of the pandemic.\nContinued research modeling the spread of infectious disease is necessary in order to help reduce many of the negative effects of COVID and other disease. We hope that with effective modeling of new infections, governments, institutions, and individuals may be able to implement effective interventions to slow the spread of COVID and reduce the serious health and social consequences of infectious disease.\nReferences 1. CDC. Healthcare Workers. https://www.cdc.gov/coronavirus/2019-ncov/hcp/durationisolation.html, February 2020. 2. CDC. Omicron variant: What you need to know. May 2022. https://www.cdc.gov/coronavirus/2019ncov/variants/omicron-variant.html. 3. Thomas Hale, Noam Angrist, Rafael Goldszmidt, Beatriz Kira, Anna Petherick, Toby Phillips, Samuel Webster, Emily Cameron-Blake, Laura Hallas, Saptarshi Majumdar, and Helen Tatlow. A global panel database of pandemic policies (Oxford COVID-19 Government Response Tracker). Nature Human Behaviour, 5(4):529–538, April 2021. ISSN 2397-3374. doi: 10.1038/s41562-021-01079-8. 4. Lucas Rod ́es-Guirao Cameron Appel Charlie Giattino Esteban Ortiz-Ospina Joe Hasell Bobbie Macdonald Diana Beltekian Hannah Ritchie, Edouard Mathieu and Max Roser. Coronavirus pandemic (covid-19). Our World in Data, 2020. https://ourworldindata.org/coronavirus. 5. Theodore Kypraios, Peter Neal, and Dennis Prangle. A tutorial introduction to Bayesian inference for stochastic epidemic models using Approximate Bayesian Computation. Mathematical Biosciences, 287:42–53, May 2017. ISSN 0025-5564. doi:10.1016/j.mbs.2016.07.001. 6. Lili Wang, Yiwang Zhou, Jie He, Bin Zhu, Fei Wang, Lu Tang, Michael Kleinsasser, Daniel Barker, Marisa C. Eisenberg, and Peter X.K. Song. An epidemiological forecast model and software assessing interventions on the covid-19 epidemic in china. Journal of Data Science, 18(3):409–432, 2021. ISSN 1680-743X. doi: 10.6339/JDS.20200718(3).0003. 7. Phitchayapak Wintachai and Kiattisak Prathom. Stability analysis of SEIR model related to efficiency of vaccines for COVID-19 situation. Heliyon, 7(4), April 2021. ISSN 2405-8440. doi: 10.1016/j.heliyon.2021.e06812 ","permalink":"http://localhost:1313/stats/bayesian_sir/","summary":"This project was co-authored by Heather Johnston and Youngwoo Kwon $$ \\gdef{\\boldphi}{\\boldsymbol{\\phi}} $$\nIntroduction COVID-19, or Coronavirus Disease 2019, was declared a pandemic by the World Health Organization on March 11, 2020. The disease originated in December 2019 in China and quickly spread around the world. Compared with the previous epidemics like Ebola and SARS, COVID-19 has a relatively low mortality rate, but spreads faster with numerous mildly symptomatic cases.","title":"Bayesian Methods for the Estimation of Infection and Recovery Rates of an Epidemic from Stochastic SIR Data"},{"content":" ನೀ ಹೀಂಗ ನೋಡಬ್ಯಾಡ ನನ್ನ\nನೀ ಹೀಂಗ ನೋಡಿದರ ನನ್ನ ತಿರುಗಿ ನಾ ಹ್ಯಾಂಗ ನೋಡಲೇ ನಿನ್ನ\nಸಂಸಾರ ಸಾಗರದಾಗ ಲೆಕ್ಕವಿರದಷ್ಟು ದುಃಖದ ಬಂಡಿ\nನಾ ಬಲ್ಲೆ ನನಗೆ ಗೊತ್ತಿಲ್ಲದಿದ್ದರೂ ಎಲ್ಲಿ ಆಚೆಯಾ ದಂಡಿ\nಮಲಗಿರುವ ಕೂಸು ಮಲಗಿರಲಿ ಅಲ್ಲಿ, ಮುಂದಿನದು ದೇವರ ಚಿತ್ತ\nನಾ ತಡೀಲಾರೆ ಅದು, ಯಾಕ ನೋಡತೀ ಮತ್ತ ಮತ್ತ ಇತ್ತ?\nತಂಬಲs ಹಾಕದ ತುಂಬ ಕೆಂಪು ಗಿಡಗಡಕಹಣ್ಣಿನ ಹಾಂಗ\nಇದ್ದಂಥ ತುಟಿಯ ಬಣ್ಣೆತ್ತ ಹಾರಿತು? ಯಾವ ಗಾಳಿಗೆ, ಹೀಂಗ\nಈ ಗದ್ದ, ಗಲ್ಲ, ಹಣಿ, ಕಣ್ಣುಕಂಡು ಮಾರೀಗೆ ಮಾರಿಯ ರೀತಿ\nಸಾವನs ತನ್ನ ಕೈ ಸವರಿತಲ್ಲಿ, ಬಂತೆsನಗ ಇಲ್ಲದ ಭೀತಿ\nಧಾರೀಲೆ ನೆನೆದ ಕೈ ಹಿಡಿದೆ ನೀನು, ತಣ್ಣsಗ ಅಂತ ತಿಳಿದು\nಬಿಡವೊಲ್ಲಿ ಇನ್ನುನೂ, ಬೂದಿಮುಚ್ಚಿದ ಕೆಂಡ ಇದಂತ ಹೊಳೆದು\nಮುಗಿಲsನ ಕಪ್ಪರಿಸಿ ನೆಲಕ ಬಿದ್ದರ ನೆಲಕ ನೆಲಿ ಎಲ್ಲಿನ್ನs\nಆ ಗಾದಿ ಮಾತು ನಂಬಿ, ನಾನು ದೇವರಂತ ತಿಳಿದಿಯೇನ ನೀ ನನ್ನ.\nಇಬ್ಬನ್ನಿ ತೊಳೆದರೂ ಹಾಲು ಮೆತ್ತಿದಾ ಕವಳಿಕಂಟಿಯಾ ಹಣ್ಣು\nಹೊಳೆ ಹೊಳೆವ ಹಾಂಗ ಕಣ್ಣಿರುವ ಹೆಣ್ಣ, ಹೇಳು ನಿನ್ನವೇನ ಈ ಕಣ್ಣು?\nದಿಗಿಲಾಗಿ ಅನ್ನತದ ಜೀವ ನಿನ್ನ ಕಣ್ಣಾರೆ ಕಂಡು ಒಮ್ಮಿಗಿಲs\nಹುಣ್ಣವೀ ಚಂದಿರನ ಹೆಣ ಬಂತೊ ಮುಗಿಲಾಗ ತೇಲತ ಹಗಲ!\nನಿನ ಕಣ್ಣಿನ್ಯಾಗ ಕಾಲೂರಿ ಮಳೆಯು, ನಡ ನಡಕ ಹುಚ್ಚನಗಿ ಯಾಕ?\nಹನಿ ಒಡೆಯಲಿಕ್ಕೆ ಬಂದಂಥ ಮೋಡ ತಡಧಾಂಗ ಗಾಳಿಯ ನೆವಕ\nಅತ್ತಾರ ಅತ್ತುಬಿಡು, ಹೊನಲು ಬರಲಿ, ನಕ್ಯಾಕ ಮರಸತೀ ದುಕ್ಕ?\nಎವೆಬಡಿಸಿ ಕೆಡವು, ಬಿರಿಗಣ್ಣು ಬ್ಯಾಡ, ತುಟಿಕಚ್ಚಿ ಹಿಡಿಯದಿರು ಬಿಕ್ಕ\n– ಅಂಬಿಕಾತನಯದತ್ತ\n","permalink":"http://localhost:1313/kannada/nee_hinga_nodabeda/","summary":"ನೀ ಹೀಂಗ ನೋಡಬ್ಯಾಡ ನನ್ನ\nನೀ ಹೀಂಗ ನೋಡಿದರ ನನ್ನ ತಿರುಗಿ ನಾ ಹ್ಯಾಂಗ ನೋಡಲೇ ನಿನ್ನ\nಸಂಸಾರ ಸಾಗರದಾಗ ಲೆಕ್ಕವಿರದಷ್ಟು ದುಃಖದ ಬಂಡಿ\nನಾ ಬಲ್ಲೆ ನನಗೆ ಗೊತ್ತಿಲ್ಲದಿದ್ದರೂ ಎಲ್ಲಿ ಆಚೆಯಾ ದಂಡಿ\nಮಲಗಿರುವ ಕೂಸು ಮಲಗಿರಲಿ ಅಲ್ಲಿ, ಮುಂದಿನದು ದೇವರ ಚಿತ್ತ\nನಾ ತಡೀಲಾರೆ ಅದು, ಯಾಕ ನೋಡತೀ ಮತ್ತ ಮತ್ತ ಇತ್ತ?\nತಂಬಲs ಹಾಕದ ತುಂಬ ಕೆಂಪು ಗಿಡಗಡಕಹಣ್ಣಿನ ಹಾಂಗ\nಇದ್ದಂಥ ತುಟಿಯ ಬಣ್ಣೆತ್ತ ಹಾರಿತು? ಯಾವ ಗಾಳಿಗೆ, ಹೀಂಗ\nಈ ಗದ್ದ, ಗಲ್ಲ, ಹಣಿ, ಕಣ್ಣುಕಂಡು ಮಾರೀಗೆ ಮಾರಿಯ ರೀತಿ\nಸಾವನs ತನ್ನ ಕೈ ಸವರಿತಲ್ಲಿ, ಬಂತೆsನಗ ಇಲ್ಲದ ಭೀತಿ\nಧಾರೀಲೆ ನೆನೆದ ಕೈ ಹಿಡಿದೆ ನೀನು, ತಣ್ಣsಗ ಅಂತ ತಿಳಿದು","title":"ತಿರುಗಿ ನಾ ಹ್ಯಾಂಗ ನೋಡಲೇ ನಿನ್ನ?"},{"content":" Photograph credits: Vulkan Olmez Introduction In the last two decades, there have been significant developments in understanding the anatomy of the brain for mental illnesses like Major Depressive Disorder, Anxiety Disorder, Bipolar Disorder, Schizophrenia, Personality Disorder, etc. Most of these disorders, along with alcohol and drug abuse are studied together to analyze the possible coexistence and the quantification of the correlation. One such study by Muester, et. al[1], estimates high rates of co-occurring mental illnesses and substance use disorders at around 50%. While studying the psychiatric and pharmacological aspects of such disorders is very important for treating, studying the environmental and genetic predispositions of a mental illness can help regulate and mitigate its effect at an early stage.\nEven though it\u0026rsquo;s estimated that 10% of the world population suffers from some psychiatric illness, as pointed out by WHO, one of the most common barriers for the countries to provide healthcare to the citizens with mental health disorders is the cultural and social stigma against mental illnesses. In addition to this, there are problems concerning lack of education, limited availability of mental health professionals, and limited affordability (poverty)[2]. Also, systematic repression, starvation, human rights abuse can contribute to mental illnesses like PTSD, anxiety, and depression, as pointed out in a study by Riadh T. Abed[3].\nStudies involving socio-economic triggers like income, history of abuse, genetic history, employment status, etc can never be causal, as the random assignments of the test subjects to study the causation of the aforementioned parameters are unethical[4]. Instead, the associative studies for the same can give us valuable insights regarding the parameters involved.\nIn this research, I analyze the association of schizophrenia, depression, anxiety, bipolar disorder, alcohol abuse, drug abuse, and suicide, with the GDP per capita, Human Development Index (HDI), unemployment rate, and population. The analysis employs clustering to stratify the samples into developed and underdeveloped countries. The illness rates are then compared with bootstrapped hypothesis tests and the inferences are thus drawn. The following report contains five sections: Data, Methodology, Simulation, Analysis, and Discussion.\nData The dataset regarding the prevalence of mental illnesses was obtained by the Institute for Health Metrics and Evaluation[5] which contains the percentage of people in a country suffering from a particular mental illness from 1990 to 2017. I also obtained multiple Human Development Report records from the United Nations Development Programme[6] containing GDP per capita, population, suicide rate, unemployment rate, and Human Development Index (HDI) for different countries from 1991 to 2017. These records were merged using pandas for python by handling exceptions and missing values. The data was then cleaned up to avoid redundancies and rows with missing values were eliminated.\nIn addition to these, I added a few ‘flag’ variables that can be used for manual stratification for further analysis. The basis for the stratification is to differentiate developed and underdeveloped countries, to analyze the mental health landscape in a paired fashion. These flags are boolean features that are obtained by implementing a cutoff for the variables HDI, GDP Per capita, and Unemployment Rates. Along with the same, we also cut off the year in which the data was recorded. The cutoff value of 2010 was chosen for the same as 90% of the countries in the world had access to the internet connection. This would serve as an indicator to see if the development of technology impacts the variables of interest. Similarly, a cutoff of 6% for the unemployment rate and $3,000[7] for GDP per capita was chosen.\nlibrary(\u0026#34;GGally\u0026#34;) library(ggplot2) library(dplyr) library(boot) library(gridExtra) library(parallel) library(vegan) library(cluster) library(coin) options(warn=-1) set.seed(84884) cl \u0026lt;- makeCluster(detectCores()) df \u0026lt;- read.csv(\u0026#39;absolute_grand_final.csv\u0026#39;) df$unef \u0026lt;- ifelse(df$unemp \u0026lt;= 6, TRUE, FALSE) df$hdif \u0026lt;- ifelse(df$HDI \u0026gt;= 0.55, TRUE, FALSE) df$intf \u0026lt;- ifelse(df$Year \u0026gt;= 2010, TRUE, FALSE) df$gdppcf \u0026lt;- ifelse(df$GDP \u0026gt;=3000, TRUE, FALSE) The dataset thus obtained has 17 variables and 3749 observations. The curated list of variables are as follows: Name of the Country (Country)\nYear of recording (Year) Percentage of the population with Schizophrenia (schiz_perc) Percentage of the population with Bipolar Disorder (bipolar _perc) Percentage of the population with Anxiety (anxiety_perc) Percentage of the population with Depression (depr_perc) Percentage of the population with Drug Abuse (drug_perc) Percentage of the population with Alcohol Abuse (alc_perc) Unemployment rate (unemp) Number of suicide per 100,000 people (suicide_tt) The population of the country (pop) GDP per capita of the country (GDP) Human Development Index (HDI) Flag for unemployment. (\u0026gt;6 is recorded as underdeveloped) (unef) Flag for HDI. (\u0026lt;0.55 is recorded as underdeveloped) (hdif) Flag for the internet. (\u0026lt;2010 is recorded as underdeveloped) (intf) Flag for GDP (\u0026lt; 3000 is considered as underdeveloped) (gdppcf) Correlation of all numerical variables\nFor inspection, we can plot the correlation plot between the numerical variables. The method uses Pearson’s correlation method internally. Since this is not stratified, the global inference for correlation may suffer a high bias[8].\ndf_num \u0026lt;- subset(df, select = -c(Country, unef, hdif, intf, gdppcf)) ggcorr( df_num, geom = \u0026#34;tile\u0026#34;, max_size = 6, size = 3.5, hjust = 0.75, #angle = -45, color = \u0026#34;grey50\u0026#34;, label = TRUE, label_size = 3, label_round = 2, label_alpha = TRUE, palette = \u0026#34;\u0026#34; ) To see if our manual stratification method shows any significant boundary between the developed and underdeveloped countries, we can plot densities using the stratification flag as a label. If we use the GDP flag to check the growth metrics of a country, we get the following plot.\na \u0026lt;- ggplot(df, aes(x = HDI, color = hdif, fill = hdif)) + geom_density(alpha = 0.5)+theme_minimal()+xlab(\u0026#34;HDI\u0026#34;) b \u0026lt;- ggplot(df, aes(x = GDP, color = hdif, fill = hdif)) + geom_density(alpha = 0.5)+xlim(c(0, 20000))+xlab(\u0026#34;GDP Per Capita\u0026#34;)+theme_minimal() c \u0026lt;- ggplot(df, aes(x = pop, color = hdif, fill = hdif)) + geom_density(alpha = 0.5)+xlim(c(0, 1000000))+xlab(\u0026#34;Population\u0026#34;)+theme_minimal() d \u0026lt;- ggplot(df, aes(x = unemp, color = hdif, fill = hdif)) + geom_density(alpha = 0.5)+xlab(\u0026#34;Unemployment Rate\u0026#34;)+theme_minimal() grid.arrange(a, b, c, d, nrow=2, ncol=2) Density plot of developed and undeveloped countries using HDI based clustering\nMethodology My first aim is to find out if the mental illnesses rates are mutually independent of one another. For example, mental illnesses like bipolar disorder and anxiety disorders are found to be highly correlated[9]. Before we employ a method to compare independence, we need to see if the data follows a normal distribution.\na \u0026lt;- ggplot(df, aes(x = bipolar_perc)) + geom_density(alpha = 0.5)+theme_minimal()+xlab(\u0026#34;Bipolar Disorder\u0026#34;)+ylab(\u0026#34;Density\u0026#34;) b \u0026lt;- ggplot(df, aes(x = anxiety_perc)) + geom_density(alpha = 0.5)+xlab(\u0026#34;Anxiety\u0026#34;)+ylab(\u0026#34;Density\u0026#34;)+theme_minimal() c \u0026lt;- ggplot(df, aes(x = schiz_perc)) + geom_density(alpha = 0.5)+xlab(\u0026#34;Schizophrenia\u0026#34;)+ylab(\u0026#34;Density\u0026#34;)+theme_minimal() d \u0026lt;- ggplot(df, aes(x = depr_perc)) + geom_density(alpha = 0.5)+xlab(\u0026#34;Depression\u0026#34;)+ylab(\u0026#34;Density\u0026#34;)+theme_minimal() grid.arrange(a, b, c, d, nrow=2, ncol=2) Density plot of different mental illnesses\nThe data looks like it deviates from the normal distribution quite a bit. To quantify the normality, we can use the Shapiro-Wilk test, with the test statistic given by,\n$$ W = \\frac{\\left( \\sum_{i=1}^n a_i x_{(i)}\\right)^2}{\\sum_{i=1}^n (x_i-\\bar{x})^2} $$\nwhere \\( x_{(i)} \\) is the order statistic and x is the mean of the sample.\ndis \u0026lt;- c(\u0026#34;schiz_perc\u0026#34;, \u0026#34;bipolar_perc\u0026#34;, \u0026#34;anxiety_perc\u0026#34;, \u0026#34;depr_perc\u0026#34;) c \u0026lt;- t(combn(dis,2)) for (i in dis){ shapiro.test(as.numeric(unlist(df[i]))) } The Shapiro-Wilk tests evaluated on all of the illnesses have the \\( p \u0026lt; 2.2e-16 \\) which implies that we reject the hypothesis that the variables are normally distributed. Since we know that the variables are not normally distributed, we can use a non-parametric, distribution-free test for independence like a permutation test.\nIn this research, I have employed a two-sided permutation test for independence, and we can also quantify the association by bootstrapped analysis of the correlation. Here, I use the Pearson’s correlation given by,\n$$ r = \\frac{ \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) }{% \\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}} $$\nThe corresponding hypothesis test is given by,\n$$ H_0:r_{xy} = 0 \\\\ H_A:r_{xy} \\ne 0 $$\nThe second aim of the research is to analyze the mean difference in the rate of mental illnesses between the developed and underdeveloped countries. Now the criteria for classifying the countries into developed and underdeveloped are not well defined. According to the United Nations’ M49 standards for classification[14][15], “The designations “developed” and “developing” are intended for statistical convenience and do not necessarily express a judgment about the stage reached by a particular country or area in the development process.”\nTo analyze the effect of a country on the prevalence of a mental illness, we can differentiate the developed countries from underdeveloped countries concerning a cutoff like GDP Per Capita and then analyze the paired mean tests. But in doing so, we would be ignoring the other metrics which may contain useful information for the cut-off. We need a better way of clustering the variables[10].\nIn this research, I employ two methods for the stratification of the countries into the aforementioned classes:\nHDI based stratification Clustering-based stratification In the HDI-based stratification, we employ a cut-off value for the HDI, and we classify the countries into one of the two classes depending on their HDI being higher or lower than the cutoff. In the clustering-based stratification, we use clustering methods for all the growth parameters, and we interpret the clusters qualitatively to see if they form a well-defined boundary concerning the individual growth parameters.\nIn the research by Elizabeth Tipton[11], the method discussed employs dividing the population samples into strata using unsupervised clustering for estimating boundary conditions concerning multiple variables. One of the proposed methods employs k-means clusters to stratify the samples. As opposed to euclidean distance, which is commonly used for the k-means clustering, we use the Gower distance metric. This is a general coefficient of similarity, which was proposed by Gower[12] in 1971, given by,\n$$ d_{ii}^g = \\frac{\\sum_{h=1}^p w_{ii\u0026rsquo;h} h_{ii\u0026rsquo;h}}{\\sum{h=1}^p w_{ii\u0026rsquo;h}} $$\nFor the two methods of stratification, I perform the bootstrap simulations to test for the difference in means for each mental illness and suicide rate. For each simulation, I generate the 95% confidence interval to test the hypothesis. If the interval at 95% confidence contains the null value (0), I do not reject the null hypothesis.\nSimulations In this research, I implement the two-sided permutation test for independence with 10000 Monte-Carlo samples and approximate density estimation.\ndis \u0026lt;- c(\u0026#34;schiz_perc\u0026#34;, \u0026#34;bipolar_perc\u0026#34;, \u0026#34;anxiety_perc\u0026#34;, \u0026#34;depr_perc\u0026#34;) c \u0026lt;- t(combn(dis,2)) for (i in 1:dim(c)[1]){ print(independence_test(as.formula(paste(c[i,1], \u0026#39;~\u0026#39; , c[i,2])), df, distribution = approximate(10000))) } The results are as follows:\nSchizophrenia Bipolar Disorder Anxiety Disorder Depression Schizophrenia - \u0026lt; 1e-04 \u0026lt; 1e-04 \u0026lt; 1e-04 Bipolar Disorder \u0026lt; 1e-04 - \u0026lt; 1e-04 Anxiety Disorder \u0026lt; 1e-04 \u0026lt; 1e-04 - Depression \u0026lt; 1e-04 \u0026lt; 1e-04 \u0026lt; 1e-04 This shows that we can reject the null hypothesis that the variables are independent. The problem boils down to finding the 95% confidence interval for the correlation coefficient to see if it contains 0, which is the null value. As mentioned before, we use bootstrapping with 1000 bootstrap replications. Bootstrapping is implemented with parallelization for the effective utilization of the CPU cycles. If we use bootstrapping for every pair of mental illnesses and suicide rates, we get the following confidence intervals.\nget_corr \u0026lt;- function(x, index){ a = names(x)[1] b = names(x)[2] x_sample \u0026lt;- x[index,] return(cor(x_sample[a],x_sample[b])) } do_boot \u0026lt;- function(x){ print(names(x)) boot_corr \u0026lt;- boot(x, statistic = get_corr, R = 1000, parallel = \u0026#39;snow\u0026#39;, cl=cl) print(boot.ci(boot_corr, type = c(\u0026#34;basic\u0026#34;), conf = 0.95)) cat(\u0026#34;\\n\\n\u0026#34;) } for (i in 1:dim(c)[1]){ do_boot(df[c[i,]]) } Schizophrenia Bipolar Disorder Anxiety Disorder Depression Suicide Schizophrenia 1 (0.3392,0.4170) (0.4686,0.5265) (0.0657,0.1354) (-0.1433, -0.1025) Bipolar Disorder (0.3392,0.4170) 1 (0.6775,0.7106) (0.1433,0.2009) (-0.2320, -0.1834) Anxiety Disorder (0.4686,0.5265) (0.6775,0.7106) 1 (0.2911,0.3418) (-0.2036, -0.1571) Depression (0.0657,0.1354) (0.1433,0.2009) (0.2911,0.3418) 1 (0.3294,0.3828) Suicide (-0.1433, -0.1025) (-0.2320, -0.1834) (-0.2036, -0.1571) (0.3294,0.3828) 1 While we see that mental illnesses are mutually positively correlated, we can also see that illnesses like bipolar disorder, schizophrenia, and anxiety disorders are strongly negatively correlated with the suicide rate. Furthermore, all the variables are mutually correlated to each other, hence we reject the hypothesis that the variables are uncorrelated. When the same test for correlation is repeated for the growth metrics, we get the following confidence intervals.\ndis \u0026lt;- c(\u0026#34;pop\u0026#34;, \u0026#34;GDP\u0026#34;, \u0026#34;HDI\u0026#34;) c \u0026lt;- t(combn(dis,2)) get_corr \u0026lt;- function(x, index){ a = names(x)[1] b = names(x)[2] x_sample \u0026lt;- x[index,] return(cor(x_sample[a],x_sample[b])) } do_boot \u0026lt;- function(x){ print(names(x)) boot_corr \u0026lt;- boot(x, statistic = get_corr, R = 1000, parallel = \u0026#39;snow\u0026#39;, cl=cl) print(boot.ci(boot_corr, type = c(\u0026#34;basic\u0026#34;), conf = 0.95)) cat(\u0026#34;\\n\\n\u0026#34;) } for (i in 1:dim(c)[1]){ do_boot(df[c[i,]]) } Population HDI GDP Per Capita Population 1 (-0.0573, -0.0214) (-0.0921, -0.0700) HDI (-0.0573, -0.0214) 1 ( 0.6749, 0.7057) GDP Per Capita (-0.0921, -0.0700) ( 0.6749, 0.7057) 1 The simulation result shows that the population is negatively correlated with HDI and GDP Per Capita. HDI and GDP Per Capita are positively correlated. We reject the null for the same aforementioned reasons. Now for the difference in mean hypothesis, as mentioned before, I use two methods of stratification. Firstly for the HDI-based stratification, I employed the flags that I set previously. We can plot the strata for the year 2000 to understand the clustering.\nvalcol \u0026lt;- c(\u0026#34;schiz_perc\u0026#34;,\u0026#34;bipolar_perc\u0026#34;,\u0026#34;anxiety_perc\u0026#34;,\u0026#34;drug_perc\u0026#34;,\u0026#34;depr_perc\u0026#34;,\u0026#34;alc_perc\u0026#34;,\u0026#34;suicide_tt\u0026#34;) ggplot(df[df$Year == 2000,], aes(GDP,HDI,color=hdif)) + geom_point(size = 0.1, stroke = 0, shape = 16) + geom_text(aes(label=Country), size=3)+theme_minimal() Scatter plot of the stratification using a cutoff on HDI\nWe can plot the density for the illnesses and the growth parameters based on this criterion.\na \u0026lt;- ggplot(df, aes(x = HDI, color = strata, fill = strata)) + geom_density(alpha = 0.5)+theme_classic() b \u0026lt;- ggplot(df, aes(x = GDP, color = strata, fill = strata)) + geom_density(alpha = 0.5)+xlim(c(0, 20000))+theme_classic() c \u0026lt;- ggplot(df, aes(x = pop, color = strata, fill = strata)) + geom_density(alpha = 0.5)+theme_classic()+xlim(c(0, 1000000)) d \u0026lt;- ggplot(df, aes(x = unemp, color = strata, fill = strata)) + geom_density(alpha = 0.5)+theme_classic() grid.arrange(a, b, c, d, nrow=2, ncol=2) Density plot of the growth metrics using HDI based clustering\na \u0026lt;- ggplot(df, aes(x = bipolar_perc, color = hdif, fill = hdif)) + geom_density(alpha = 0.5)+theme_classic() b \u0026lt;- ggplot(df, aes(x = schiz_perc, color = hdif, fill = hdif)) + geom_density(alpha = 0.5)+theme_classic() c \u0026lt;- ggplot(df, aes(x = anxiety_perc, color = hdif, fill = hdif)) + geom_density(alpha = 0.5)+theme_classic() d \u0026lt;- ggplot(df, aes(x = depr_perc, color = hdif, fill = hdif)) + geom_density(alpha = 0.5)+theme_classic() grid.arrange(a, b, c, d, nrow=2, ncol=2) Density plot of the mental illnesses using HDI based clustering\nggplot(df, aes(x = suicide_tt, fill = hdif)) + geom_density(alpha = 0.5)+theme_classic() Density plot of the suicide rates using HDI based clustering\nNow we perform clustering-based stratification. The growth metrics, along with the flags are used as inputs for clustering. The numerical values are normalized to satisfy the equal variance condition required by the k-means algorithm. Gower metric works for both categorical and numerical variables[13] and hence my binary flag variables don’t have to be numerical and scaled. We can plot the strata for the year 2000 to understand the clustering.\nu \u0026lt;- df %\u0026gt;% select(c(\u0026#34;Year\u0026#34;,\u0026#34;GDP\u0026#34;,\u0026#34;HDI\u0026#34;,\u0026#34;unef\u0026#34;,\u0026#34;hdif\u0026#34;,\u0026#34;intf\u0026#34;,\u0026#34;gdppcf\u0026#34;,\u0026#34;pop\u0026#34;)) %\u0026gt;% mutate_if(is.numeric, scale) %\u0026gt;% vegdist(method = \u0026#34;gower\u0026#34;) %\u0026gt;% pam(k=2, diss=TRUE) df$strata \u0026lt;- ifelse(u$clustering == 1, yes=TRUE, no=FALSE) ggplot(df[df$Year == 2000,], aes(GDP,HDI,color=strata)) + geom_point(size = 0.1, stroke = 0, shape = 16) + geom_text(aes(label=Country), size=3) Scatter plot of the stratification using Gower clustering\nClustered variables are used as stratification labels and we can plot the densities of the growth metrics with the stratification label fills.\na \u0026lt;- ggplot(df, aes(x = HDI, color = strata, fill = strata)) + geom_density(alpha = 0.5)+theme_classic() b \u0026lt;- ggplot(df, aes(x = GDP, color = strata, fill = strata)) + geom_density(alpha = 0.5)+xlim(c(0, 20000))+theme_classic() c \u0026lt;- ggplot(df, aes(x = pop, color = strata, fill = strata)) + geom_density(alpha = 0.5)+theme_classic()+xlim(c(0, 1000000)) d \u0026lt;- ggplot(df, aes(x = unemp, color = strata, fill = strata)) + geom_density(alpha = 0.5)+theme_classic() grid.arrange(a, b, c, d, nrow=2, ncol=2) Density plot of growth metrics using Gower clustering\nWe can see that the red density curve represents higher GDP and higher HDI and hence, generally covers the developed countries. Now we can also plot the densities of the illness according to this strata.\nDensity plot of mental illnesses using Gower clustering\nThe density plot shows some distinctions in the distributions of mental illnesses. To obtain quantified differences and the respective confidence intervals for the paired means, we can perform a simulation using bootstrapping.\na \u0026lt;- ggplot(df, aes(x = bipolar_perc, color = strata, fill = strata)) + geom_density(alpha = 0.5)+theme_classic() b \u0026lt;- ggplot(df, aes(x = schiz_perc, color = strata, fill = strata)) + geom_density(alpha = 0.5)+theme_classic() c \u0026lt;- ggplot(df, aes(x = anxiety_perc, color = strata, fill = strata)) + geom_density(alpha = 0.5)+theme_classic() d \u0026lt;- ggplot(df, aes(x = depr_perc, color = strata, fill = strata)) + geom_density(alpha = 0.5)+theme_classic() grid.arrange(a, b, c, d, nrow=2, ncol=2) Density plot of the suicide rates using Gower clustering\nWe can see that the two clustering methods perform similarly. But, using a cluster that is representative of multiple variables seems to be more intuitive. Hence, I proceed with Gower clustering for bootstrapped hypothesis testing of paired means.\nWe use the statistic \\( T = x_u-x_d \\) where \\( x_u \\) is the sample mean of the underdeveloped countries and \\( x_d \\) is the sample mean of the developed countries. We perform bootstrapping with 1000 bootstrap replicates.\nget_meandiff \u0026lt;- function(x, index, a){ x_sample \u0026lt;- x[index,] return(mean(as.matrix(x_sample[!x_sample$strata,][a]))-mean(as.matrix(x_sample[x_sample$strata,][a]))) } valcol \u0026lt;- c(\u0026#34;schiz_perc\u0026#34;,\u0026#34;bipolar_perc\u0026#34;,\u0026#34;anxiety_perc\u0026#34;,\u0026#34;drug_perc\u0026#34;,\u0026#34;depr_perc\u0026#34;,\u0026#34;alc_perc\u0026#34;,\u0026#34;suicide_tt\u0026#34;) for (i in valcol){ print(i) boot_corr \u0026lt;- boot(df, statistic = get_meandiff, a = i, R = 1000, parallel = \u0026#39;snow\u0026#39;, cl=cl) print(boot.ci(boot_corr, type = c(\u0026#34;basic\u0026#34;), conf = 0.95)) cat(\u0026#34;\\n\\n\u0026#34;) } The result of the bootstrap analysis with a 95% confidence interval for the difference of mean between the underdeveloped and the developed countries for the variables of interest are as follows:\nVariable of Interest Confidence Interval Schizophrenia ( 0.0381, 0.0428 ) Bipolar Disorder ( 0.1572, 0.1721 ) Anxiety Disorder ( 0.8179, 0.9341 ) Depression (-0.1927, -0.1093 ) Suicide (-2.097, -1.166 ) Analysis The outcome of the permutation test for independence suggests that the prevalence of mental illnesses suggests that there is evidence to show that the existence of mental illnesses is mutually correlated. This is pointed out in the research by Elliott, Marta, et al.[16] which shows that there are many common explanatory factors for mental illnesses. In the quantification of the dependence using correlation, we obtain the following result at a 95% confidence interval.\nSchizophrenia Bipolar Disorder Anxiety Disorder Depression Suicide Schizophrenia 1 (0.3392,0.4170) (0.4686,0.5265) (0.0657,0.1354) (-0.1433, -0.1025) Bipolar Disorder (0.3392,0.4170) 1 (0.6775,0.7106) (0.1433,0.2009) (-0.2320, -0.1834) Anxiety Disorder (0.4686,0.5265) (0.6775,0.7106) 1 (0.2911,0.3418) (-0.2036, -0.1571) Depression (0.0657,0.1354) (0.1433,0.2009) (0.2911,0.3418) 1 (0.3294,0.3828) Suicide (-0.1433, -0.1025) (-0.2320, -0.1834) (-0.2036, -0.1571) (0.3294,0.3828) 1 While it’s evident that mental illnesses are mutually positively correlated, Bipolar Disorder, Anxiety Disorders, and Schizophrenia seem to be negatively correlated with suicide. This is contradictory to the study by Novick, Danielle M, et al.[17] which suggests that around 32.4% of the population with Bipolar II and 36.3% of the population with Bipolar I attempt suicide and between 4% and 19% of the people suffering from the illness complete suicide. A similar study estimates a rate of 4.9% for schizophrenia[18] and 3.4% for anxiety disorders[19].\nIn fact, a study by Arsenault-Lapierre, Geneviève et al.[20] suggests that 90% of the deaths by suicide have had a history of mental illness. In my analysis, I have a strong negative correlation between mental illnesses and suicide rates which contradicts the previous studies.\nComing to the hypothesis testing for the difference in paired means (u-d) of the mental illnesses and the suicide rates results in the following confidence intervals:\nVariable of Interest Confidence Interval Schizophrenia ( 0.0381, 0.0428 ) Bipolar Disorder ( 0.1572, 0.1721 ) Anxiety Disorder ( 0.8179, 0.9341 ) Depression (-0.1927, -0.1093 ) Suicide (-2.097, -1.166 ) This implies that, for mental illnesses like Schizophrenia, Bipolar Disorder, and Anxiety disorder, the difference between the average population suffering from illnesses in underdeveloped countries with the ones in developed countries is positive.\nIn other words, the percentage of the population suffering from these illnesses is greater in the developed countries. But, the rates of depression and suicide are higher in underdeveloped countries. We need to analyze this disparity further.\nIn the research by Kopinak, et al.[21], the problems that the developing and underdeveloped countries face in the mental health sector on diagnosis, treatment, affordability, and education are discussed. In addition to this, it’s found that in many countries, Diagnostic Statistical Manual (DSM) which provides guidelines for mental illnesses is considered flawed which results in the under-diagnosis of the illnesses. When it comes to depression disorders, most of them are diagnosed by a primary care physician[22]. Disorders like schizophrenia, bipolar disorder, anxiety disorders, personality disorders, etc are diagnosed by a specialist (typically a psychiatrist). Hence, the poor diagnosis and an inaccurate representation of the proportion of people suffering from these disorders can be attributed to the lack of availability, development, and research in the mental health landscape in those countries.\nWhile there are strict guidelines to report suicide to the United Nations by its member countries[23], the same does not exist for mental illnesses as the diagnosis requires human resources specializing in the same[24]. It’s estimated that in underdeveloped and developing countries, around 76% of people with mental illnesses receive no treatment for their disorder.\nDiscussion In this literature, I have addressed the following research questions –\nIs there any evidence that the mental illness rates for different illnesses are correlated? What is the mean difference in mental illnesses and suicide rates in developed and underdeveloped countries? The data was obtained from Human Development Report records from the United Nations Development Programme. The data was cleaned up and variables of interest were retained and additional flag variables were created. To analyze the normality condition, the Shapiro-Wilk test was performed on each variable of the dataset. Following the non-normality of the dataset, a permutation test – a non-parametric distribution-free method was proposed to assess the independence of the variables (first research question). To assess the difference of paired means (second research question) the data were clustered to reduce the dimensionality of the growth factors followed by a bootstrap-based hypothesis testing.\nIn summary, we can attribute the lower percentages of mental illnesses like bipolar disorder, anxiety disorders, schizophrenia, etc, to the poor quality of data arising from the inadequate diagnostic systems. And more accurate data for suicides and depression owing to the feasibility in diagnosing and reporting in underdeveloped countries.\nOne of the assumptions made in the following analysis was that the samples are independent. But, the countries evolving over time can be considered a stochastic process and we need time-series methods for clustering the data. We can also employ hypothesis testing and parameter estimation for the time-series features to project the development of a country pertaining to the mental health landscape.\nReferences 1. Drake, Robert E et al. “Management of persons with co-occurring severe mental illness and substance use disorder: program implications.” 2. World psychiatry : official journal of the World Psychiatric Association (WPA) vol. 6,3 (2007): 131-6. 3. Kohrt, Brandon A et al. “The Role of Communities in Mental Health Care in Low- and Middle-Income Countries: A Meta-Review of Components and 4. Competencies.” International journal of environmental research and public health vol. 15,6 1279. 16 Jun. 2018, doi:10.3390/ijerph15061279 5. Abed, Riadh T. “Tyranny and Mental Health.” British Medical Bulletin, vol. 72, no. 1, Oxford University Press (OUP), Jan. 2004, pp. 1–13. Crossref, doi:10.1093/bmb/ldh037. 6. Jain, Shobhit et al. “Ethics in Psychiatric Research: Issues and Recommendations.” Indian journal of psychological medicine vol. 39,5 (2017): 558-565. doi:10.4103/IJPSYM.IJPSYM_131_17 7. Global Burden of Disease Collaborative Network. Global Burden of Disease Study 2017 (GBD 2017) Results. Seattle, United States: Institute for Health Metrics and Evaluation (IHME), 2018. 8. HDRO calculations based on data from UNDESA (2019a), UNESCO Institute for Statistics (2020), United Nations Statistics Division (2020b), World Bank (2020a), Barro and Lee (2018) and IMF (2020) 9. World Bank, Constant GDP per capita for Low Income Countries [NYGDPPCAPKDLIC], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/NYGDPPCAPKDLIC, March 21, 2021. 10. Kaplan, Robert M et al. “Big data and large sample size: a cautionary note on the potential for bias.” Clinical and translational science vol. 7,4 (2014): 342-6. doi:10.1111/cts.12178 11. Simon, Naomi M et al. “Anxiety disorder comorbidity in bipolar disorder patients: data from the first 500 participants in the Systematic Treatment Enhancement Program for Bipolar Disorder (STEP-BD).” The American journal of psychiatry vol. 161,12 (2004): 2222-9. doi:10.1176/appi.ajp.161.12.2222 12. Elliott, Michael R. “A simple method to generate equal-sized homogenous strata or clusters for population-based sampling.” Annals of epidemiology vol. 21,4 (2011): 290-6. doi:10.1016/j.annepidem.2010.11.016 13. Tipton, Elizabeth. “Stratified Sampling Using Cluster Analysis: A Sample Selection Strategy for Improved Generalizations From Experiments.” Evaluation Review, vol. 37, no. 2, Apr. 2013, pp. 109–139, doi:10.1177/0193841X13516324. 14. Gower, J. C. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics, vol. 27, no. 4, 1971, pp. 857–871. JSTOR, www.jstor.org/stable/2528823. Accessed 21 Mar. 2021. 15. Hummel, Manuela et al. “Clustering of samples and variables with mixed-type data.” PloS one vol. 12,11 e0188274. 28 Nov. 2017, doi:10.1371/journal.pone.0188274 16. “Standard Country and Area Codes Classifications (M49): Developed Regions”. United Nations Statistics Division. Archived from the original on 11 July 2017. Retrieved 13 May 2017. 17. “United Nations Statistics Division- Standard Country and Area Codes Classifications (M49)”. Unstats.un.org. Retrieved 15 January 2014. 18. Elliott, Marta, et al. “Subjective Accounts of the Causes of Mental Illness in the USA.” International Journal of Social Psychiatry, vol. 58, no. 6, Nov. 2012, pp. 562–567, doi:10.1177/0020764011415207. 19. Novick, Danielle M et al. “Suicide attempts in bipolar I and bipolar II disorder: a review and meta-analysis of the evidence.” Bipolar disorders vol. 12,1 (2010): 1-9. doi:10.1111/j.1399-5618.2009.00786.x 20. Hor, Kahyee, and Mark Taylor. “Suicide and schizophrenia: a systematic review of rates and risk factors.” Journal of psychopharmacology (Oxford, England) vol. 24,4 Suppl (2010): 81-90. doi:10.1177/1359786810385490 21. Nepon, Josh et al. “The relationship between anxiety disorders and suicide attempts: findings from the National Epidemiologic Survey on 22. Alcohol and Related Conditions.” Depression and anxiety vol. 27,9 (2010): 791-8. doi:10.1002/da.20674\\ 23. Arsenault-Lapierre, Geneviève et al. “Psychiatric diagnoses in 3275 suicides: a meta-analysis.” BMC psychiatry vol. 4 37. 4 Nov. 2004, doi:10.1186/1471-244X-4-37 24. Kopinak, Janice Katherine. “Mental Health in Developing Countries: Challenges and Opportunities in Introducing Western Mental Health System in Uganda.” International journal of MCH and AIDS vol. 3,1 (2015): 22-30. 25. Ng, Chung Wai Mark et al. “Major depression in primary care: making the diagnosis.” Singapore medical journal vol. 57,11 (2016): 591-597. doi:10.11622/smedj.2016174 26. “Suicide – Challenges and Obstacles.” World Health Organization, World Health Organization, www.who.int/news-room/fact-sheets/detail/suicide. 27. Who.int. 2021. Mental disorders. [online] Available at: \u0026lt;https://www.who.int/news-room/fact-sheets/detail/mental-disorders\u0026gt; [Accessed 23 April 2021]. ","permalink":"http://localhost:1313/stats/analysis_of_mental_illness/","summary":"Photograph credits: Vulkan Olmez Introduction In the last two decades, there have been significant developments in understanding the anatomy of the brain for mental illnesses like Major Depressive Disorder, Anxiety Disorder, Bipolar Disorder, Schizophrenia, Personality Disorder, etc. Most of these disorders, along with alcohol and drug abuse are studied together to analyze the possible coexistence and the quantification of the correlation. One such study by Muester, et. al[1], estimates high rates of co-occurring mental illnesses and substance use disorders at around 50%.","title":"Analysis of Prevalence of Mental Illnesses and Suicide in Different Countries"},{"content":" ಘಮ ಘಮ ಘಮಾಡಿಸ್ತಾವ ಮಲ್ಲಿಗೆ\nನೀ ಹೊರಟಿದ್ದೀಗ ಎಲ್ಲಿಗೆ?\nತುಳುಕ್ಯಾಡುತ್ತಾವ ತೂಕಡಿಕಿ\nಎವಿ ಅಪ್ಪುತ್ತಾವ ಕಣ್ ದುಡುಕಿ\nಕನಸು ತೇಲಿ ಬರುತ್ತಾವ ಹುಡುಕಿ\nನೀ ಹೊರಟಿದ್ದೀಗ ಎಲ್ಲಿಗೆ?\nಚಿಕ್ಕಿ ತೋರಿಸ್ತಾವ ಚಾಚಿ ಬೆರಳ\nಚಂದ್ರಮ ಕನ್ನಡಿ ಹರಳ\nಮನ ಸೋತು ಆಯಿತು ಮರುಳ\nನೀ ಹೊರಟಿದ್ದೀಗ ಎಲ್ಲಿಗೆ?\nನೆರಳಲ್ಲಾಡುತ್ತಾವ ಮರದ ಬುಡಕ್ಕ\nಕೇರಿ ತೇರಿ ನೂಗುತ್ತಾವ ದಡಕ್ಕ\nಹಿಂಗ ಬಿಟ್ಟು ಇಲ್ಲಿ ನನ್ನ ನಡಕ\nನೀ ಹೊರಟಿದ್ದೀಗ ಎಲ್ಲಿಗೆ?\nನನ್ನ ನಿನ್ನ ಒಂದತನದಾಗ\nಹಾಡು ಹುಟ್ಟಿ ಒಂದು ಮನದಾಗ\nಬೆಳದಿಂಗಳಾತು ಬನದಾಗ\nನೀ ಹೊರಟಿದ್ದೀಗ ಎಲ್ಲಿಗೆ?\nಬಂತ್ಯಾಕ ನಿನಗ ಇಂದ ಮುನಿಸು\nಬೀಳಲಿಲ್ಲ ನನಗ ಇದರ ಕನಸು\nರಾಯ ತಿಳಿಯಲಿಲ್ಲ ನಿನ್ನ ಮನಸು\nನೀ ಹೊರಟಿದ್ದೀಗ ಎಲ್ಲಿಗೆ?\n– ಅಂಬಿಕಾತನಯದತ್ತ\n","permalink":"http://localhost:1313/kannada/nee_horatiddiga_ellige/","summary":"ಘಮ ಘಮ ಘಮಾಡಿಸ್ತಾವ ಮಲ್ಲಿಗೆ\nನೀ ಹೊರಟಿದ್ದೀಗ ಎಲ್ಲಿಗೆ?\nತುಳುಕ್ಯಾಡುತ್ತಾವ ತೂಕಡಿಕಿ\nಎವಿ ಅಪ್ಪುತ್ತಾವ ಕಣ್ ದುಡುಕಿ\nಕನಸು ತೇಲಿ ಬರುತ್ತಾವ ಹುಡುಕಿ\nನೀ ಹೊರಟಿದ್ದೀಗ ಎಲ್ಲಿಗೆ?\nಚಿಕ್ಕಿ ತೋರಿಸ್ತಾವ ಚಾಚಿ ಬೆರಳ\nಚಂದ್ರಮ ಕನ್ನಡಿ ಹರಳ\nಮನ ಸೋತು ಆಯಿತು ಮರುಳ\nನೀ ಹೊರಟಿದ್ದೀಗ ಎಲ್ಲಿಗೆ?\nನೆರಳಲ್ಲಾಡುತ್ತಾವ ಮರದ ಬುಡಕ್ಕ\nಕೇರಿ ತೇರಿ ನೂಗುತ್ತಾವ ದಡಕ್ಕ\nಹಿಂಗ ಬಿಟ್ಟು ಇಲ್ಲಿ ನನ್ನ ನಡಕ\nನೀ ಹೊರಟಿದ್ದೀಗ ಎಲ್ಲಿಗೆ?\nನನ್ನ ನಿನ್ನ ಒಂದತನದಾಗ\nಹಾಡು ಹುಟ್ಟಿ ಒಂದು ಮನದಾಗ\nಬೆಳದಿಂಗಳಾತು ಬನದಾಗ\nನೀ ಹೊರಟಿದ್ದೀಗ ಎಲ್ಲಿಗೆ?\nಬಂತ್ಯಾಕ ನಿನಗ ಇಂದ ಮುನಿಸು\nಬೀಳಲಿಲ್ಲ ನನಗ ಇದರ ಕನಸು","title":"ನೀ ಹೊರಟಿದ್ದೀಗ ಎಲ್ಲಿಗೆ?"},{"content":"Implementation of the Inverse Transform Sampling The probability density function of a power law distribution is given by,\n$$f(x\\ | \\ x_m,\\alpha) = \\frac{\\alpha-1}{x_m} \\left(\\frac{x}{x_m}\\right)^{-\\alpha} \\ , x\u0026gt;x_m$$\nWe can find the cumulative density function by,\n$$ \\begin{aligned} F(x) \u0026amp;= \\int_{-\\infty}^x f(x\\ | \\ x_m,\\alpha) \\ dx \\\\ \u0026amp;= \\int_{-\\infty}^x \\frac{\\alpha-1}{x_m} \\left(\\frac{x}{x_m}\\right)^{-\\alpha} I(x\u0026gt;x_m) \\ dx \\\\ \u0026amp;= \\int_{x_m}^x \\frac{\\alpha-1}{x_m} \\left(\\frac{x}{x_m}\\right)^{-\\alpha} \\ dx \\\\ \u0026amp;= \\frac{\\alpha-1}{x_m} \\frac{1}{x_m^{-\\alpha}} \\int_{x_m}^x x^{-\\alpha} \\ dx \\\\ \u0026amp;= \\frac{\\alpha-1}{x_m^{-\\alpha+1}} \\left[\\frac{x^{-\\alpha+1}}{-\\alpha+1} \\right]_{x_m}^x \\\\ F(x) \u0026amp;= 1- \\left( \\frac{x}{x_m} \\right)^{-\\alpha+1}\\\\ \\end{aligned} $$\nFor a uniform distribution given by, $$u \\sim \\text{Uniform}[0,1]$$\ncumulative density is given by, $$F_u(u) = u$$\nTo generate power law distributions from uniform distribution,\n$$ \\begin{aligned} \u0026amp;F_u(u) = F(x) \\\\ u \u0026amp;= 1- \\left( \\frac{x}{x_m} \\right)^{-\\alpha+1} \\\\ x \u0026amp;= x_m(1-u)^{\\frac{1}{1-\\alpha}} \\end{aligned} $$\nBy fixing the parameters, we can generate x which follows the power law distribution.\nI was examining methods to improve Maximum Likelihood Estimation (MLE) for parameters of a power law sample as a part of an independent with two other students.\nFor the same, we needed methods to generate power law samples with different parameters. There are no implementations in programming languages like MATLAB, Python, R and Julia to generate Power Law distributions. As a part of my graduate course (STATS 511), I learnt generation of samples (from a given distribution) from uniform samples.\nI implemented the same in Python, R and Julia so that it can be used by anyone who deals with analysis of Power Law distribution.\nFind my code on GitHub.\n","permalink":"http://localhost:1313/stats/powerlaw_inverse_transform/","summary":"Implementation of the Inverse Transform Sampling The probability density function of a power law distribution is given by,\n$$f(x\\ | \\ x_m,\\alpha) = \\frac{\\alpha-1}{x_m} \\left(\\frac{x}{x_m}\\right)^{-\\alpha} \\ , x\u0026gt;x_m$$\nWe can find the cumulative density function by,\n$$ \\begin{aligned} F(x) \u0026amp;= \\int_{-\\infty}^x f(x\\ | \\ x_m,\\alpha) \\ dx \\\\ \u0026amp;= \\int_{-\\infty}^x \\frac{\\alpha-1}{x_m} \\left(\\frac{x}{x_m}\\right)^{-\\alpha} I(x\u0026gt;x_m) \\ dx \\\\ \u0026amp;= \\int_{x_m}^x \\frac{\\alpha-1}{x_m} \\left(\\frac{x}{x_m}\\right)^{-\\alpha} \\ dx \\\\ \u0026amp;= \\frac{\\alpha-1}{x_m} \\frac{1}{x_m^{-\\alpha}} \\int_{x_m}^x x^{-\\alpha} \\ dx \\\\ \u0026amp;= \\frac{\\alpha-1}{x_m^{-\\alpha+1}} \\left[\\frac{x^{-\\alpha+1}}{-\\alpha+1} \\right]_{x_m}^x \\\\ F(x) \u0026amp;= 1- \\left( \\frac{x}{x_m} \\right)^{-\\alpha+1}\\\\ \\end{aligned} $$","title":"Generation of Power Law Samples with Inverse Transform Sampling (Python, R and Julia)"}]